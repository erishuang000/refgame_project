# test_model.py

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def test_model(model_path, text, device="cuda"):
    print(f"\nğŸ” Loading model from: {model_path}")

    # åŠ è½½ tokenizer å’Œ model
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path, output_hidden_states=True).to(device)
    model.eval()

    # ç¼–ç æ–‡æœ¬
    inputs = tokenizer(text, return_tensors="pt").to(device)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(**inputs)

    # è·å– embedding å±‚è¾“å‡ºï¼ˆhidden_states[0]ï¼‰
    hidden_states = outputs.hidden_states  # Tuple of length (n_layers + 1)
    embedding_layer = hidden_states[0]

    print(f"âœ… Model tested successfully: {model_path}")
    print(f"ğŸ”¢ Input text: {text}")
    print(f"ğŸ“ Input token shape: {inputs['input_ids'].shape}")
    print(f"ğŸ“Š Embedding shape: {embedding_layer.shape}")  # [batch_size, seq_len, hidden_size]
    print(f"ğŸ“ˆ Number of layers (including embedding): {len(hidden_states)}")

if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # ğŸ€„ ä¸­æ–‡æ¨¡å‹ï¼ˆCPMï¼‰
    test_model("models/CPM-Generate", "ä¸€ä¸ªçº¢è‰²çš„ä¸œè¥¿ä»é«˜å¤„è½ä¸‹ã€‚", device)

    # ğŸ‡¬ğŸ‡§ è‹±æ–‡æ¨¡å‹ï¼ˆGPT2ï¼‰
    test_model("models/gpt2", "An apple fell on the ground.", device)
