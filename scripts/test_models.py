from transformers import BertTokenizer, AutoTokenizer, AutoModelForCausalLM
import torch
import os

def test_model(model_path, text, device="cuda"):
    print(f"\nğŸ” Loading model from: {model_path}")

    if "CPM" in model_path:
        # æ‰‹åŠ¨æŒ‡å®š CPM ä½¿ç”¨ SentencePiece tokenizer
        from transformers import PreTrainedTokenizerFast
        tokenizer = PreTrainedTokenizerFast(
            tokenizer_file=os.path.join(model_path, "tokenizer.json"),
            model_max_length=1024
        )
        tokenizer.add_special_tokens({"pad_token": "<pad>"})  # CPMæ¨¡å‹éœ€è¦pad token
        model = AutoModelForCausalLM.from_pretrained(model_path, output_hidden_states=True).to(device)
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path, output_hidden_states=True).to(device)

    model.eval()

    inputs = tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    hidden_states = outputs.hidden_states
    embedding_layer = hidden_states[0]

    print(f"âœ… Model tested successfully: {model_path}")
    print(f"ğŸ”¢ Input text: {text}")
    print(f"ğŸ“ Input token shape: {inputs['input_ids'].shape}")
    print(f"ğŸ“Š Embedding shape: {embedding_layer.shape}")
    print(f"ğŸ“ˆ Number of layers (including embedding): {len(hidden_states)}")

if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    test_model("models/CPM-Generate", "ä¸€ä¸ªçº¢è‰²çš„ä¸œè¥¿ä»é«˜å¤„è½ä¸‹ã€‚", device)
    test_model("models/gpt2", "An apple fell on the ground.", device)
