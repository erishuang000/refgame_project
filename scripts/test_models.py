# from transformers import AutoTokenizer, AutoModelForCausalLM
# import torch
# # from transformers import AutoTokenizer, AutoModelWithLMHead
#
# model_id = "TsinghuaAI/CPM-Generate"
# save_path = "./CPM-Generate"
#
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# model = AutoModelWithLMHead.from_pretrained(model_id)
#
# tokenizer.save_pretrained(save_path)
# model.save_pretrained(save_path)


# def test_model_cpm(model_path, input_text, device):
#     print(f"ğŸ” Loading model from: {model_path}")
#
#     tokenizer = AutoTokenizer.from_pretrained(
#         model_path,
#         trust_remote_code=True,
#         local_files_only=True
#     )
#     model = AutoModelForCausalLM.from_pretrained(
#         model_path,
#         trust_remote_code=True,
#         local_files_only=True
#     ).to(device)
#
#     inputs = tokenizer(input_text, return_tensors="pt").to(device)
#     outputs = model.generate(**inputs, max_length=50)
#     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
#
#     print("ğŸ§  Output:", decoded)


# if __name__ == "__main__":
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     test_model_cpm("/puhome/23063003r/refgame_project/models/CPM-Generate", "ä¸€ä¸ªçº¢è‰²çš„ä¸œè¥¿ä»é«˜å¤„è½ä¸‹ã€‚", device)


from transformers import AutoTokenizer, AutoModelWithLMHead, AdamW
import torch

# è·¯å¾„æ ¹æ®ä½ çš„æœ¬åœ°æ¨¡å‹ä½ç½®ä¿®æ”¹
MODEL_PATH = "/puhome/23063003r/refgame_project/models/CPM-Generate"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelWithLMHead.from_pretrained(MODEL_PATH)
model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

# å‡è®¾å¯¹è¯æ ·æœ¬
sample_text = "ç”¨æˆ·ï¼šä½ å¥½\næ¨¡å‹ï¼šä½ å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ"
inputs = tokenizer(sample_text, return_tensors="pt")
inputs = {k: v for k, v in inputs.items() if k in ["input_ids", "attention_mask"]}
inputs["labels"] = inputs["input_ids"].clone()

# æ‹¿åˆ°embeddingå±‚
embedding_before = model.transformer.wte.weight.clone().detach()

# ä¸€æ­¥å¾®è°ƒ
optimizer = AdamW(model.parameters(), lr=1e-5)
outputs = model(**inputs)
loss = outputs.loss
loss.backward()
optimizer.step()

# æ¯”è¾ƒembeddingæ”¹å˜
embedding_after = model.transformer.wte.weight.detach()
diff = torch.norm(embedding_after - embedding_before).item()

print(f"âœ… å¾®è°ƒå®Œæˆã€‚Loss = {loss.item():.4f}")
print(f"ğŸ” Embedding æ”¹å˜é‡: {diff:.6f}")
