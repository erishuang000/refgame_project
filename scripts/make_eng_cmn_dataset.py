import bz2
import tarfile
import os
import json
import random

# --- é…ç½® ---
DATA_DIR = "/puhome/23063003r/refgame_project/data/"
CMN_FILE = os.path.join(DATA_DIR, "cmn_sentences_detailed.tsv.bz2")
ENG_FILE = os.path.join(DATA_DIR, "eng_sentences.tsv.bz2")
LINKS_FILE = os.path.join(DATA_DIR, "links.tar.bz2")
OUTPUT_JSON_FILE = os.path.join(DATA_DIR, "generated_game_data.json")

NUM_CANDIDATES = 3  # æ¯ä¸ªå›åˆçš„è‹±æ–‡å€™é€‰å¥å­æ•°é‡ (1ä¸ªæ­£ç¡® + N-1ä¸ªå¹²æ‰°)
NUM_SAMPLES_TO_GENERATE = 15000 # æƒ³è¦ç”Ÿæˆçš„æ¸¸æˆå›åˆæ•°ã€‚å¦‚æœNoneï¼Œåˆ™ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç®€å•å¥å¯¹ã€‚
MIN_SENTENCE_LEN = 3 # å¥å­çš„æœ€å°è¯æ•°ï¼ˆè‹±æ–‡ï¼‰æˆ–å­—ç¬¦æ•°ï¼ˆä¸­æ–‡ï¼‰
MAX_SENTENCE_LEN = 15 # å¥å­çš„æœ€å¤§è¯æ•°ï¼ˆè‹±æ–‡ï¼‰æˆ–å­—ç¬¦æ•°ï¼ˆä¸­æ–‡ï¼‰


# --- è¾…åŠ©å‡½æ•°ï¼šè¯»å–æ–‡ä»¶ ---

def read_bz2_tsv_to_dict(filepath):
    """è¯»å–.bz2çš„tsvæ–‡ä»¶ï¼Œå¹¶æ„å»º ID -> æ–‡æœ¬ çš„å­—å…¸"""
    id_to_text = {}
    try:
        with bz2.open(filepath, 'rt', encoding='utf8') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 3: # ç¡®ä¿è‡³å°‘æœ‰IDã€è¯­è¨€ã€æ–‡æœ¬ä¸‰åˆ—
                    sentence_id = parts[0]
                    text = parts[2]
                    id_to_text[sentence_id] = text
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - {filepath}")
        return None
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ {filepath}: {e}")
        return None
    return id_to_text

def read_tar_bz2_links(filepath):
    """è¯»å–.tar.bz2ä¸­çš„links.tsvæ–‡ä»¶ï¼Œå¹¶è¿”å›é“¾æ¥å¯¹åˆ—è¡¨"""
    links = []
    try:
        with tarfile.open(filepath, 'r:bz2') as tar:
            members = tar.getmembers()
            if not members:
                print(f"è­¦å‘Š: {filepath} ä¸­æ²¡æœ‰æˆå‘˜æ–‡ä»¶ã€‚")
                return []

            # æŸ¥æ‰¾ links.tsv æˆ–ç±»ä¼¼çš„ç¬¬ä¸€ä¸ª .tsv/.csv æ–‡ä»¶
            links_member = None
            for member in members:
                if member.name.endswith('.tsv') or member.name.endswith('.csv'):
                    links_member = member
                    break

            if not links_member:
                print(f"è­¦å‘Š: åœ¨ {filepath} ä¸­æœªæ‰¾åˆ° .tsv æˆ– .csv æˆå‘˜æ–‡ä»¶ã€‚")
                return []

            extracted_file = tar.extractfile(links_member)
            if extracted_file:
                for line in extracted_file:
                    parts = line.decode('utf8').strip().split('\t')
                    if len(parts) == 2: # ç¡®ä¿æ˜¯ ID \t ID æ ¼å¼
                        links.append((parts[0], parts[1]))
            else:
                print(f"é”™è¯¯: æ— æ³•æå– {links_member.name} ä» {filepath}ã€‚")

    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - {filepath}")
        return []
    except tarfile.ReadError:
        print(f"é”™è¯¯: æ— æ³•è§£å‹.tar.bz2æ–‡ä»¶ï¼Œå¯èƒ½å·²æŸåæˆ–æ ¼å¼ä¸æ­£ç¡® - {filepath}")
        return []
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ {filepath}: {e}")
        return []
    return links

def is_simple_sentence(text, lang_code, min_len, max_len):
    """åˆ¤æ–­å¥å­æ˜¯å¦ç¬¦åˆç®€å•å¥å­æ ‡å‡†ï¼ˆåŸºäºé•¿åº¦ï¼‰"""
    if not text:
        return False
    if lang_code == 'cmn': # ä¸­æ–‡æŒ‰å­—ç¬¦æ•°
        length = len(text.replace(" ", "")) # ç§»é™¤ç©ºæ ¼è®¡ç®—å­—ç¬¦æ•°
    else: # è‹±æ–‡æŒ‰è¯æ•°
        length = len(text.split())
    return min_len <= length <= max_len

# --- ä¸»è¦æ„å»ºå‡½æ•° ---

def build_tatoeba_game_data(
    cmn_filepath, eng_filepath, links_filepath,
    num_candidates=3, num_samples=None,
    min_len=3, max_len=15
):
    print("1. è¯»å–ä¸­æ–‡å¥å­æ•°æ®...")
    cmn_id_to_text = read_bz2_tsv_to_dict(cmn_filepath)
    if cmn_id_to_text is None: return []
    print(f"   - è¯»å–åˆ° {len(cmn_id_to_text)} æ¡ä¸­æ–‡å¥å­ã€‚")

    print("2. è¯»å–è‹±æ–‡å¥å­æ•°æ®...")
    eng_id_to_text = read_bz2_tsv_to_dict(eng_filepath)
    if eng_id_to_text is None: return []
    print(f"   - è¯»å–åˆ° {len(eng_id_to_text)} æ¡è‹±æ–‡å¥å­ã€‚")

    print("3. è¯»å–é“¾æ¥æ•°æ®...")
    links = read_tar_bz2_links(links_filepath)
    if not links: return []
    print(f"   - è¯»å–åˆ° {len(links)} æ¡é“¾æ¥ã€‚")

    print("4. æ„å»ºå¹³è¡Œå¥å¯¹å¹¶ç­›é€‰ç®€å•å¥å­...")
    all_parallel_pairs = [] # å­˜å‚¨ (ä¸­æ–‡æ–‡æœ¬, è‹±æ–‡æ–‡æœ¬) å¯¹

    # å»ºç«‹ä¸€ä¸ªè‹±æ–‡å¥å­åˆ—è¡¨ï¼Œç”¨äºéšæœºæŠ½å–å¹²æ‰°é¡¹
    all_english_sentences_list = list(eng_id_to_text.values())

    for id1, id2 in links:
        text1 = cmn_id_to_text.get(id1)
        text2 = eng_id_to_text.get(id2)

        # æ£€æŸ¥æ˜¯å¦æ˜¯ ä¸­æ–‡ -> è‹±æ–‡ çš„é“¾æ¥
        if text1 and text2 and is_simple_sentence(text1, 'cmn', min_len, max_len) and is_simple_sentence(text2, 'eng', min_len, max_len):
            all_parallel_pairs.append((text1, text2))

        # æ£€æŸ¥æ˜¯å¦æ˜¯ è‹±æ–‡ -> ä¸­æ–‡ çš„é“¾æ¥ï¼ˆTatoebaé“¾æ¥æ˜¯åŒå‘çš„ï¼Œä½†ä¿é™©èµ·è§éƒ½æ£€æŸ¥ï¼‰
        text1_eng = eng_id_to_text.get(id1)
        text2_cmn = cmn_id_to_text.get(id2)
        if text1_eng and text2_cmn and is_simple_sentence(text2_cmn, 'cmn', min_len, max_len) and is_simple_sentence(text1_eng, 'eng', min_len, max_len):
             # é¿å…é‡å¤æ·»åŠ ï¼Œå› ä¸ºlinksæ–‡ä»¶å¯èƒ½å·²ç»åŒ…å«åŒå‘é“¾æ¥
            if (text2_cmn, text1_eng) not in all_parallel_pairs:
                all_parallel_pairs.append((text2_cmn, text1_eng))

    print(f"   - æ‰¾åˆ° {len(all_parallel_pairs)} æ¡ç¬¦åˆç®€å•å¥å­æ ‡å‡†çš„å¹³è¡Œå¥å¯¹ã€‚")

    print("5. ç”Ÿæˆæ¸¸æˆå›åˆæ•°æ®...")
    game_data = []

    # ç¡®å®šè¦å¤„ç†çš„æ ·æœ¬æ•°é‡
    actual_num_samples = num_samples if num_samples is not None and num_samples <= len(all_parallel_pairs) else len(all_parallel_pairs)

    # éšæœºé€‰æ‹©è¦å¤„ç†çš„å¹³è¡Œå¥å¯¹ï¼Œä»¥é¿å…æ¯æ¬¡éƒ½å¤„ç†ç›¸åŒçš„å‰Nä¸ªï¼Œç‰¹åˆ«æ˜¯å½“æ•°æ®é‡å¾ˆå¤§æ—¶
    selected_parallel_pairs = random.sample(all_parallel_pairs, actual_num_samples)

    for i, (target_cn, correct_en) in enumerate(selected_parallel_pairs):
        candidates = [correct_en] # æ·»åŠ æ­£ç¡®ç­”æ¡ˆ

        # æ”¶é›†å¹²æ‰°é¡¹
        distractors = []
        # ä»æ‰€æœ‰è‹±æ–‡å¥å­ä¸­éšæœºé€‰æ‹©ï¼Œç›´åˆ°è¾¾åˆ°æ‰€éœ€æ•°é‡ï¼Œä¸”ä¸ä¸æ­£ç¡®ç­”æ¡ˆé‡å¤
        while len(distractors) < (num_candidates - 1):
            random_en = random.choice(all_english_sentences_list)
            if random_en != correct_en and random_en not in distractors:
                distractors.append(random_en)

        candidates.extend(distractors)

        # æ‰“ä¹±å€™é€‰å¥å­ï¼Œå¹¶æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ–°ç´¢å¼•
        random.shuffle(candidates)
        correct_index = candidates.index(correct_en)

        game_round = {
            "target_sentence_chinese_raw": target_cn,
            "correct_english_sentence_raw": correct_en,
            "candidate_english_sentences_raw": candidates,
            "correct_candidate_index": correct_index
        }
        game_data.append(game_round)

        if (i + 1) % 100 == 0:
            print(f"   - å·²ç”Ÿæˆ {i + 1}/{actual_num_samples} è½®æ¸¸æˆæ•°æ®ã€‚")

    print(f"   - æœ€ç»ˆç”Ÿæˆ {len(game_data)} è½®æ¸¸æˆæ•°æ®ã€‚")
    return game_data

# --- æ‰§è¡Œæ„å»ºå¹¶ä¿å­˜ ---
if __name__ == "__main__":
    print("--- å¼€å§‹æ„å»ºæ¸¸æˆæ•°æ®åº“ ---")

    # æ„å»ºæ¸¸æˆæ•°æ®
    generated_game_data = build_tatoeba_game_data(
        CMN_FILE, ENG_FILE, LINKS_FILE,
        num_candidates=NUM_CANDIDATES,
        num_samples=NUM_SAMPLES_TO_GENERATE, # æ ¹æ®éœ€è¦è°ƒæ•´ç”Ÿæˆçš„æ ·æœ¬æ•°
        min_len=MIN_SENTENCE_LEN,
        max_len=MAX_SENTENCE_LEN
    )

    if generated_game_data:
        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(generated_game_data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ‰ æˆåŠŸå°†æ¸¸æˆæ•°æ®ä¿å­˜åˆ°: {OUTPUT_JSON_FILE}")
    else:
        print("\nâŒ æœªèƒ½ç”Ÿæˆæ¸¸æˆæ•°æ®ã€‚è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå†…å®¹ã€‚")
