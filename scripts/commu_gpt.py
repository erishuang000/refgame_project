import torch
from transformers import AutoTokenizer, GPT2Model
import json
import torch.nn.functional as F
from torch.optim import AdamW
import os # å¯¼å…¥ os æ¨¡å—ç”¨äºè·¯å¾„æ“ä½œ

# --- é…ç½® ---
MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/gpt2"
DATA_FILE = "/ubsnhome/23063003r/refgame_project/data/generated_game_data.json"
OUTPUT_DIR = "/ubsnhome/23063003r/refgame_project/output/"
D_HIDDEN = 1600 
# D_HIDDEN = 768 # GPT-2çš„éšè—å±‚ç»´åº¦

# å®šä¹‰å¥–åŠ±å’Œæƒ©ç½šå€¼ï¼ˆå¯ä»¥æ ¹æ®å®éªŒè°ƒæ•´ï¼‰
REWARD_CORRECT = 0.1 # çŒœå¯¹æ—¶çš„â€œå¥–åŠ±â€å¼ºåº¦ (å‡å°æŸå¤±)
PENALTY_WRONG = 1.0Â  # çŒœé”™æ—¶çš„â€œæƒ©ç½šâ€å¼ºåº¦ (å¢å¤§æŸå¤±)

# --- Agent B (Listener - GPT-2) è§†è§’ ---

# 1. åŠ è½½ GPT-2 æ¨¡å‹å’Œ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = GPT2Model.from_pretrained(MODEL_PATH) # ä½¿ç”¨ GPT2Model è·å–å¥å­åµŒå…¥
model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

# --- IMPORTANT: Set pad_token for GPT-2 tokenizer ---
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 2. æ‰©å±• GPT-2 çš„ Tokenizer ä»¥æ”¯æŒä¸­æ–‡
all_chinese_chars_in_corpus = set()
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        full_game_data = json.load(f)
    for entry in full_game_data:
        all_chinese_chars_in_corpus.update(list(entry['target_sentence_chinese_raw']))
except Exception as e:
    print(f"âŒ é”™è¯¯åŠ è½½æ•°æ®ä»¥æ”¶é›†ä¸­æ–‡ç¬¦å·: {e}")
    all_chinese_chars_in_corpus = set("ä¸€ä¸ªè‹¹æœæ‰åˆ°äº†åœ°ä¸Šã€‚çŒ«è·³åˆ°äº†æ¡Œå­ä¸Šã€‚ä¸€è¾†çº¢è‰²çš„æ±½è½¦å¼€åœ¨è¡—ä¸Šã€‚ç‹—è¿½çƒã€‚å¤©ç©ºæ˜¯è“è‰²çš„ã€‚å¥¹åœ¨çœ‹ä¹¦ã€‚ç¡æ²™å‘ã€‚å­©å­ä»¬åœ¨å…¬å›­ç©ã€‚å¤ªé˜³ä»ä¸œæ–¹å‡èµ·ã€‚å–œæ¬¢å¬éŸ³ä¹ã€‚å’–å•¡å¾ˆçƒ«ã€‚æˆ‘é¥¿äº†æƒ³åƒä¸œè¥¿ã€‚")


tokenizer.add_special_tokens({'additional_special_tokens': list(all_chinese_chars_in_corpus)})
model.resize_token_embeddings(len(tokenizer)) # è°ƒæ•´ embedding å±‚å¤§å°ä»¥é€‚åº”æ–°è¯æ±‡è¡¨

print(f"âœ… GPT-2 tokenizer å·²æ‰©å±•ï¼Œæ–°çš„è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
print(f"âœ… GPT-2 æ¨¡å‹ Embedding å±‚å·²è°ƒæ•´ã€‚")

# 3. éªŒè¯ D_HIDDEN ä¸æ¨¡å‹éšè—å±‚ç»´åº¦
# <<< --- IMPORTANT: è¿™é‡Œéœ€è¦éªŒè¯æ¨¡å‹å®é™…çš„ hidden_size ---
# model.config.hidden_size åº”è¯¥å°±æ˜¯ n_embdï¼Œä¹Ÿå°±æ˜¯ 1600
assert D_HIDDEN == model.config.hidden_size, f"D_HIDDEN ({D_HIDDEN}) must match GPT-2's hidden_size ({model.config.hidden_size}) for direct use."

# 4. ä¼˜åŒ–å™¨
optimizer = AdamW(model.parameters(), lr=1e-5)

# --- å‡†å¤‡è®°å½•æ•°æ® ---
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

per_round_metrics = [] # å­˜å‚¨æ¯è½®çš„è¯¦ç»†æŒ‡æ ‡

# --- æ¨¡æ‹Ÿå¤šè½®æ¸¸æˆ ---
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        all_game_rounds = json.load(f)
except FileNotFoundError:
    print(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ '{DATA_FILE}' æœªæ‰¾åˆ°ã€‚è¯·åˆ›å»ºå®ƒã€‚")
    exit()
except Exception as e:
    print(f"âŒ é”™è¯¯åŠ è½½æ•°æ®: {e}")
    exit()

print(f"\n--- å‡†å¤‡è¿›è¡Œ {len(all_game_rounds)} è½®æ¸¸æˆ ---")

total_loss_sum = 0.0
correct_predictions_count = 0
total_rounds = len(all_game_rounds)
print("total_rounds: ",total_rounds)

# å¾ªç¯è¿›è¡Œæ¯ä¸€è½®æ¸¸æˆ
for i, game_round in enumerate(all_game_rounds):
    if (i + 1) % 100 == 0 or i == 0 or (i + 1) == total_rounds:
        print(f"\n--- æ¸¸æˆå›åˆ {i + 1}/{total_rounds} ---")
        print(f"ğŸ¯ ç›®æ ‡ä¸­æ–‡å¥å­ (CPM 'è¯´'): {game_round['target_sentence_chinese_raw']}")
        print(f"ğŸ“š å€™é€‰è‹±æ–‡å¥å­ (Agent B é€‰æ‹©): {game_round['candidate_english_sentences_raw']}")
        print(f"âœ… æ­£ç¡®ç´¢å¼•: {game_round['correct_candidate_index']}")

    # 6. Agent A (CPM è§†è§’) 'è¯´' (æä¾›ä¸­æ–‡å¥å­ä½œä¸ºä¹±ç æº)
    cpm_spoken_chinese_sentence = game_round['target_sentence_chinese_raw']

    # 7. Agent B (GPT-2) å¤„ç†ä¸­æ–‡ä¹±ç è¾“å…¥
    embedding_before = model.wte.weight.clone().detach() # wte æ˜¯ Word Token Embeddings

    # --- ä¿®æ­£ä¸­æ–‡ä¹±ç è¾“å…¥ Tokenizer è°ƒç”¨ ---
    # `tokenizer()` æœŸæœ›å­—ç¬¦ä¸²åˆ—è¡¨è¿›è¡Œæ‰¹å¤„ç†ã€‚å¯¹äºå•å¥ï¼Œä¹Ÿè¦åŒ…è£…æˆåˆ—è¡¨ã€‚
    inputs_cn_symbolic = tokenizer([cpm_spoken_chinese_sentence], return_tensors="pt", padding=True, truncation=True)
    outputs_cn_symbolic = model(**inputs_cn_symbolic)
    semantic_vector_B_from_A = outputs_cn_symbolic.last_hidden_state[:, 0, :] # å–ç¬¬ä¸€ä¸ªtokençš„éšè—çŠ¶æ€

    # 8. Agent B å¤„ç†è‹±æ–‡å€™é€‰å¥å­
    # --- ä¿®æ­£è‹±æ–‡å€™é€‰å¥å­ Tokenizer è°ƒç”¨ ---
    # ç¡®ä¿ candidates åˆ—è¡¨ä½œä¸ºæ‰¹æ¬¡è¾“å…¥ç»™ tokenizer
    inputs_en_candidates = tokenizer(game_round['candidate_english_sentences_raw'], return_tensors="pt", padding=True, truncation=True)

    semantic_vectors_B_candidates = [] # åˆ—è¡¨æ¸…ç©ºï¼Œå°†ç›´æ¥ä» outputs_en_candidates è·å–

    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    inputs_cn_symbolic.to(model.device) # Move to device inside loop
    inputs_en_candidates.to(model.device) # Move to device inside loop

    outputs_en_candidates = model(**inputs_en_candidates)
    # å–æ¯ä¸ªå€™é€‰å¥å­çš„ç¬¬ä¸€ä¸ªtokençš„éšè—çŠ¶æ€ä½œä¸ºå¥å­è¡¨ç¤º
    semantic_vectors_B_candidates_batch = outputs_en_candidates.last_hidden_state[:, 0, :]

    # æ­¤æ—¶ semantic_vectors_B_candidates_batch çš„å½¢çŠ¶æ˜¯ (num_candidates, D_HIDDEN)
    # ç¡®ä¿å…¶ç»´åº¦ç¬¦åˆ listener_mse_reciprocal_loss çš„ (batch_size, num_candidates, D_HIDDEN)
    # å› ä¸ºè¿™é‡Œæ˜¯å•æ ·æœ¬å¾ªç¯ï¼Œbatch_size=1ï¼Œæ‰€ä»¥ unsqueeze(0)
    semantic_vectors_B_candidates = semantic_vectors_B_candidates_batch.unsqueeze(0)


    # 9. Agent B çŒœæµ‹ (è®¡ç®—ç›¸ä¼¼åº¦å¹¶é¢„æµ‹)
    # semantic_vector_B_from_A: (1, D_HIDDEN)
    # semantic_vectors_B_candidates: (1, num_candidates, D_HIDDEN)
    # ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—éœ€è¦è°ƒæ•´ dim å‚æ•°
    similarities = F.cosine_similarity(
        semantic_vector_B_from_A.unsqueeze(1), # (1, 1, D_HIDDEN) for broadcasting
        semantic_vectors_B_candidates,         # (1, num_candidates, D_HIDDEN)
        dim=2 # æ²¿ç€ D_HIDDEN ç»´åº¦è®¡ç®—ç›¸ä¼¼åº¦
    ).squeeze(1) # ç»“æœå½¢çŠ¶ (1, num_candidates) -> squeeze(1) to (num_candidates,)

    predicted_index = torch.argmax(similarities).item()

    if (i + 1) % 100 == 0 or i == 0 or (i + 1) == total_rounds:
        print(f"ğŸ¤” ç›¸ä¼¼åº¦å¾—åˆ† (è¶Šé«˜è¶Šç›¸ä¼¼): {similarities.tolist()}")
        print(f"ğŸ”® Agent B çŒœæµ‹çš„ç´¢å¼•: {predicted_index}")

    # 10. åé¦ˆä¸æƒé‡æ›´æ–° (Agent B å­¦ä¹ )
    # --- ä¿®æ­£ correct_index_tensor dtype å’Œ device ---
    correct_index_tensor = torch.tensor([game_round['correct_candidate_index']], device=model.device, dtype=torch.long)

    # --- Listener MSE Reciprocal Loss ---
    # semantic_vector_B_from_A: (1, D_HIDDEN)
    # semantic_vectors_B_candidates: (1, num_candidates, D_HIDDEN)
    # correct_index_tensor: (1,) (already long)

    # æ³¨æ„ï¼šlistener_mse_reciprocal_loss æœŸæœ›çš„ input_A å’Œ candidates_B å½¢çŠ¶æ˜¯ (batch_size, D_HIDDEN) å’Œ (batch_size, num_candidates, D_HIDDEN)
    # åœ¨è¿™é‡Œï¼Œç”±äºæ˜¯å•æ ·æœ¬å¾ªç¯ï¼Œå®ƒä»¬å·²ç»ç¬¦åˆè¿™ä¸ªæ‰¹æ¬¡å½¢çŠ¶
    base_loss = listener_mse_reciprocal_loss(
        semantic_vector_B_from_A,
        semantic_vectors_B_candidates,
        correct_index_tensor
    )

    is_correct = (predicted_index == game_round['correct_candidate_index']) # æ¯”è¾ƒPython int
    if is_correct:
        loss = base_loss * (1 - REWARD_CORRECT)
        outcome_message = f"ğŸ‰ Agent B çŒœå¯¹å•¦ï¼æŸå¤±è°ƒæ•´ç³»æ•°: {(1 - REWARD_CORRECT):.2f}"
    else:
        loss = base_loss * PENALTY_WRONG
        outcome_message = f"ğŸ’” Agent B çŒœé”™äº†ï¼æŸå¤±è°ƒæ•´ç³»æ•°: {PENALTY_WRONG:.2f}"

    print(outcome_message)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # 11. æ¯”è¾ƒ Embedding å˜åŒ–
    embedding_after = model.wte.weight.detach()
    diff = torch.norm(embedding_after - embedding_before).item()

    total_loss_sum += loss.item()
    if is_correct:
        correct_predictions_count += 1
    if (i + 1) % 100 == 0 or i == 0 or (i + 1) == total_rounds:
        print(f"ğŸ“‰ æœ¬è½®æ¸¸æˆæœ€ç»ˆæŸå¤±: {loss.item():.4f}")
        print(f"ğŸ” Embedding (word token embeddings) æ”¹å˜é‡: {diff:.6f}")
        print(f"âœ¨ Agent B æœ€ç»ˆçŒœæµ‹ç»“æœ: {is_correct}")

    # --- è®°å½•æœ¬è½®æ•°æ® ---
    round_data = {
        "round_idx": i + 1,
        "chinese_sentence": game_round['target_sentence_chinese_raw'],
        "correct_english_sentence": game_round['correct_english_sentence_raw'],
        "candidate_english_sentences": game_round['candidate_english_sentences_raw'],
        "correct_candidate_idx": game_round['correct_candidate_index'],
        "predicted_index": predicted_index,
        "similarities": similarities.tolist(), # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        "is_correct_prediction": is_correct,
        "base_loss": base_loss.item(),
        "final_loss": loss.item(),
        "embedding_diff_norm": diff
    }
    per_round_metrics.append(round_data)

# --- è®­ç»ƒç»“æŸï¼Œæ±‡æ€»ç»“æœå¹¶ä¿å­˜ ---
print("\n--- è®­ç»ƒæ€»ç»“ ---")
final_accuracy_percentage = (correct_predictions_count / total_rounds * 100) if total_rounds > 0 else 0
print(f"æ€»è½®æ•°: {total_rounds}")
print(f"å¹³å‡æŸå¤±: {total_loss_sum / total_rounds:.4f}")
print(f"çŒœå¯¹è½®æ•°: {correct_predictions_count}")
print(f"å‡†ç¡®ç‡: {final_accuracy_percentage:.2f}%")

# --- ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶ ---
summary_metrics = {
    "total_rounds": total_rounds,
    "final_average_loss": total_loss_sum / total_rounds,
    "final_correct_count": correct_predictions_count,
    "final_accuracy_percentage": final_accuracy_percentage
}

output_data = {
    "summary_metrics": summary_metrics,
    "per_round_metrics": per_round_metrics
}

output_file_path = os.path.join(OUTPUT_DIR, "training_results_15000.json")
with open(output_file_path, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"\nğŸ‰ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")


# --- Listener MSE Reciprocal Loss å‡½æ•°å®šä¹‰ (éœ€è¦åœ¨ä¸»è„šæœ¬ä¸­æå‰å®šä¹‰æˆ–å¯¼å…¥) ---
def listener_mse_reciprocal_loss(
    semantic_vector_from_agent_A: torch.Tensor,
    semantic_vectors_candidates_B: torch.Tensor,
    correct_candidate_index: torch.Tensor, # ç¡®ä¿è¿™é‡Œæ˜¯ LongTensor
    epsilon: float = 1e-8 # ç”¨äºæ•°å€¼ç¨³å®šçš„å°å¸¸æ•°
) -> torch.Tensor:
    """
    è®¡ç®— Listener Lossï¼Œé‡‡ç”¨è®ºæ–‡ä¸­ (EMERGENT TRANSLATION IN MULTI-AGENT COMMUNICATION)
    æè¿°çš„ MSE å€’æ•°å¯¹æ•°å½¢å¼ã€‚
    Args:
        semantic_vector_from_agent_A (torch.Tensor): Agent Aï¼ˆä¸­æ–‡ä¹±ç ï¼‰çš„è¯­ä¹‰å‘é‡ã€‚å½¢çŠ¶: (batch_size, D_HIDDEN)
        semantic_vectors_candidates_B (torch.Tensor): Agent B å€™é€‰è‹±æ–‡å¥å­çš„è¯­ä¹‰å‘é‡é›†åˆã€‚å½¢çŠ¶: (batch_size, num_candidates, D_HIDDEN)
        correct_candidate_index (torch.Tensor): æ­£ç¡®å€™é€‰å¥å­çš„ç´¢å¼•ã€‚å½¢çŠ¶: (batch_size,)
        epsilon (float): ç”¨äºæ•°å€¼ç¨³å®šçš„å°å¸¸æ•°ã€‚
    Returns:
        torch.Tensor: è®¡ç®—å‡ºçš„æŸå¤±å€¼ã€‚
    """
    # print(f"DEBUG: listener_mse_reciprocal_loss input shapes & dtypes:")
    # print(f"  semantic_vector_from_agent_A: {semantic_vector_from_agent_A.shape}, {semantic_vector_from_agent_A.dtype}")
    # print(f"  semantic_vectors_candidates_B: {semantic_vectors_candidates_B.shape}, {semantic_vectors_candidates_B.dtype}")
    # print(f"  correct_candidate_index: {correct_candidate_index.shape}, {correct_candidate_index.dtype}") # ç¡®ä¿æ˜¯ torch.long

    expanded_vector_A = semantic_vector_from_agent_A.unsqueeze(1)
    # print(f"DEBUG: expanded_vector_A shape: {expanded_vector_A.shape}")

    squared_diff = (expanded_vector_A - semantic_vectors_candidates_B).pow(2)
    # print(f"DEBUG: squared_diff shape: {squared_diff.shape}")

    mse_distances = squared_diff.sum(dim=-1)
    # print(f"DEBUG: mse_distances shape: {mse_distances.shape}")

    logits = 1 / (mse_distances + epsilon)
    # print(f"DEBUG: logits shape: {logits.shape}")

    loss = F.cross_entropy(logits, correct_candidate_index) # correct_candidate_index å¿…é¡»æ˜¯ torch.long

    return loss
