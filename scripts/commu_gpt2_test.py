import torch
from transformers import AutoTokenizer, GPT2Model
import json
import torch.nn.functional as F
from torch.optim import AdamW

# --- é…ç½® ---
MODEL_PATH = "/puhome/23063003r/refgame_project/models/gpt2" # æœ¬åœ° GPT-2 æ¨¡å‹è·¯å¾„
# æ³¨æ„ï¼šè¿™é‡Œä¿®æ”¹ä¸ºä½ çš„ data.json è·¯å¾„
DATA_FILE = "/hpc2/puhome/23063003r/refgame_project/data/test_data.json" # æ¨¡æ‹Ÿæ•°æ®åº“æ–‡ä»¶
D_HIDDEN = 768 # GPT-2çš„éšè—å±‚ç»´åº¦

# å®šä¹‰å¥–åŠ±å’Œæƒ©ç½šå€¼ï¼ˆå¯ä»¥æ ¹æ®å®éªŒè°ƒæ•´ï¼‰
REWARD_CORRECT = 0.1 # çŒœå¯¹æ—¶çš„â€œå¥–åŠ±â€å¼ºåº¦ (å‡å°æŸå¤±)
PENALTY_WRONG = 1.0  # çŒœé”™æ—¶çš„â€œæƒ©ç½šâ€å¼ºåº¦ (å¢å¤§æŸå¤±)

# --- Agent B (Listener - GPT-2) è§†è§’ ---

# 1. åŠ è½½ GPT-2 æ¨¡å‹å’Œ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = GPT2Model.from_pretrained(MODEL_PATH) # ä½¿ç”¨ GPT2Model è·å–å¥å­åµŒå…¥
model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

# 2. æ‰©å±• GPT-2 çš„ Tokenizer ä»¥æ”¯æŒä¸­æ–‡
# æ”¶é›†æ‰€æœ‰æ•°æ®ä¸­çš„ä¸­æ–‡å­—ç¬¦ä»¥ç¡®ä¿tokenizerè¦†ç›–
all_chinese_chars_in_corpus = set()
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        full_game_data = json.load(f)
    for entry in full_game_data:
        all_chinese_chars_in_corpus.update(list(entry['target_sentence_chinese_raw']))
except Exception as e:
    print(f"âŒ é”™è¯¯åŠ è½½æ•°æ®ä»¥æ”¶é›†ä¸­æ–‡ç¬¦å·: {e}")
    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤çš„æ±‰å­—é›†ä»¥é˜²ç¨‹åºä¸­æ–­ï¼Œä½†å®é™…è®­ç»ƒå¯èƒ½ä¸å®Œæ•´
    all_chinese_chars_in_corpus = set("ä¸€ä¸ªè‹¹æœæ‰åˆ°äº†åœ°ä¸Šã€‚çŒ«è·³åˆ°äº†æ¡Œå­ä¸Šã€‚ä¸€è¾†çº¢è‰²çš„æ±½è½¦å¼€åœ¨è¡—ä¸Šã€‚ç‹—è¿½çƒã€‚å¤©ç©ºæ˜¯è“è‰²çš„ã€‚å¥¹åœ¨çœ‹ä¹¦ã€‚ç¡æ²™å‘ã€‚å­©å­ä»¬åœ¨å…¬å›­ç©ã€‚å¤ªé˜³ä»ä¸œæ–¹å‡èµ·ã€‚å–œæ¬¢å¬éŸ³ä¹ã€‚å’–å•¡å¾ˆçƒ«ã€‚æˆ‘é¥¿äº†æƒ³åƒä¸œè¥¿ã€‚")


tokenizer.add_special_tokens({'additional_special_tokens': list(all_chinese_chars_in_corpus)})
model.resize_token_embeddings(len(tokenizer)) # è°ƒæ•´ embedding å±‚å¤§å°ä»¥é€‚åº”æ–°è¯æ±‡è¡¨

print(f"âœ… GPT-2 tokenizer å·²æ‰©å±•ï¼Œæ–°çš„è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
print(f"âœ… GPT-2 æ¨¡å‹ Embedding å±‚å·²è°ƒæ•´ã€‚")

# 3. éªŒè¯ D_HIDDEN ä¸æ¨¡å‹éšè—å±‚ç»´åº¦
assert D_HIDDEN == model.config.hidden_size, "D_HIDDEN must match GPT-2's hidden_size for direct use."

# 4. ä¼˜åŒ–å™¨
optimizer = AdamW(model.parameters(), lr=1e-5)

# --- æ¨¡æ‹Ÿå¤šè½®æ¸¸æˆ ---

# 5. ä»æ¨¡æ‹Ÿæ•°æ®åº“åŠ è½½æ‰€æœ‰æ¸¸æˆæ•°æ®
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        all_game_rounds = json.load(f)
except FileNotFoundError:
    print(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ '{DATA_FILE}' æœªæ‰¾åˆ°ã€‚è¯·åˆ›å»ºå®ƒã€‚")
    exit()
except Exception as e:
    print(f"âŒ é”™è¯¯åŠ è½½æ•°æ®: {e}")
    exit()

print(f"\n--- å‡†å¤‡è¿›è¡Œ {len(all_game_rounds)} è½®æ¸¸æˆ ---")

total_loss_sum = 0.0
correct_predictions_count = 0
total_rounds = len(all_game_rounds)

# å¾ªç¯è¿›è¡Œæ¯ä¸€è½®æ¸¸æˆ
for i, game_round in enumerate(all_game_rounds):
    print(f"\n--- æ¸¸æˆå›åˆ {i + 1}/{total_rounds} ---")
    print(f"ğŸ¯ ç›®æ ‡ä¸­æ–‡å¥å­ (CPM 'è¯´'): {game_round['target_sentence_chinese_raw']}")
    print(f"ğŸ“š å€™é€‰è‹±æ–‡å¥å­ (Agent B é€‰æ‹©): {game_round['candidate_english_sentences_raw']}")
    print(f"âœ… æ­£ç¡®ç´¢å¼•: {game_round['correct_candidate_index']}")

    # 6. Agent A (CPM è§†è§’) 'è¯´' (æä¾›ä¸­æ–‡å¥å­ä½œä¸ºä¹±ç æº)
    cpm_spoken_chinese_sentence = game_round['target_sentence_chinese_raw']

    # 7. Agent B (GPT-2) å¤„ç†ä¸­æ–‡ä¹±ç è¾“å…¥
    # æ‹¿åˆ° embedding å±‚ï¼Œç”¨äºæ¯”è¾ƒå˜åŒ–
    embedding_before = model.wte.weight.clone().detach() # wte æ˜¯ Word Token Embeddings

    # å°†ä¸­æ–‡å¥å­æ‹†åˆ†ä¸ºå­—ç¬¦ï¼Œå¹¶ç”¨ GPT-2 çš„ tokenizer å¤„ç†
    inputs_cn_symbolic = tokenizer(cpm_spoken_chinese_sentence, return_tensors="pt")

    # è·å–ä¸­æ–‡ä¹±ç åºåˆ—çš„è¯­ä¹‰è¡¨ç¤º
    outputs_cn_symbolic = model(**inputs_cn_symbolic)
    semantic_vector_B_from_A = outputs_cn_symbolic.last_hidden_state[:, 0, :] # å–ç¬¬ä¸€ä¸ªtokençš„éšè—çŠ¶æ€

    # 8. Agent B å¤„ç†è‹±æ–‡å€™é€‰å¥å­
    semantic_vectors_B_candidates = []
    for eng_sentence in game_round['candidate_english_sentences_raw']:
        inputs_en = tokenizer(eng_sentence, return_tensors="pt")
        outputs_en = model(**inputs_en)
        vec_en = outputs_en.last_hidden_state[:, 0, :]
        semantic_vectors_B_candidates.append(vec_en)

    semantic_vectors_B_candidates = torch.cat(semantic_vectors_B_candidates, dim=0)

    # 9. Agent B çŒœæµ‹ (è®¡ç®—ç›¸ä¼¼åº¦å¹¶é¢„æµ‹)
    similarities = F.cosine_similarity(semantic_vector_B_from_A, semantic_vectors_B_candidates, dim=1)
    predicted_index = torch.argmax(similarities).item()

    print(f"ğŸ¤” ç›¸ä¼¼åº¦å¾—åˆ† (è¶Šé«˜è¶Šç›¸ä¼¼): {similarities.tolist()}")
    print(f"ğŸ”® Agent B çŒœæµ‹çš„ç´¢å¼•: {predicted_index}")

    # 10. åé¦ˆä¸æƒé‡æ›´æ–° (Agent B å­¦ä¹ )
    correct_index_tensor = torch.tensor([game_round['correct_candidate_index']], device=similarities.device)

    # å¼•å…¥æ˜¾å¼çš„å¥–åŠ±/æƒ©ç½š
    # CrossEntropyLossçš„reduction='mean' (é»˜è®¤) æˆ– 'sum'
    # ä¸ºäº†ä¿æŒæŸå¤±é‡çº§å¯æ§ï¼Œæˆ‘ä»¬ä¿æŒé»˜è®¤çš„'mean'ï¼Œç„¶åæ ¹æ®ç»“æœè°ƒæ•´ã€‚
    base_loss = F.cross_entropy(similarities.unsqueeze(0), correct_index_tensor)

    if predicted_index == game_round['correct_candidate_index']:
        loss = base_loss * (1 - REWARD_CORRECT)
        outcome_message = f"ğŸ‰ Agent B çŒœå¯¹å•¦ï¼æŸå¤±è°ƒæ•´ç³»æ•°: {(1 - REWARD_CORRECT):.2f}"
        is_correct = True
    else:
        loss = base_loss * PENALTY_WRONG
        outcome_message = f"ğŸ’” Agent B çŒœé”™äº†ï¼æŸå¤±è°ƒæ•´ç³»æ•°: {PENALTY_WRONG:.2f}"
        is_correct = False

    print(outcome_message)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # 11. æ¯”è¾ƒ Embedding å˜åŒ– (å¹¶ç´¯åŠ ç»Ÿè®¡)
    embedding_after = model.wte.weight.detach()
    diff = torch.norm(embedding_after - embedding_before).item()

    total_loss_sum += loss.item()
    if is_correct:
        correct_predictions_count += 1
    if (i + 1) % 100 == 0 or i == 0 or (i + 1) == total_rounds:
        print(f"ğŸ“‰ æœ¬è½®æ¸¸æˆæœ€ç»ˆæŸå¤±: {loss.item():.4f}")
        print(f"ğŸ” Embedding (word token embeddings) æ”¹å˜é‡: {diff:.6f}")
        print(f"âœ¨ Agent B æœ€ç»ˆçŒœæµ‹ç»“æœ: {is_correct}")

# --- 10 è½®æ¸¸æˆç»“æŸï¼Œæ±‡æ€»ç»“æœ ---
print("\n--- 10 è½®æ¸¸æˆæ€»ç»“ ---")
print(f"æ€»è½®æ•°: {total_rounds}")
print(f"å¹³å‡æŸå¤±: {total_loss_sum / total_rounds:.4f}")
print(f"çŒœå¯¹è½®æ•°: {correct_predictions_count}")
print(f"å‡†ç¡®ç‡: {(correct_predictions_count / total_rounds * 100):.2f}%")
