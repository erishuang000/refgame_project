import torch
from transformers import AutoTokenizer, GPT2Model, AdamW
import json
import torch.nn.functional as F

# --- é…ç½® ---
MODEL_PATH = "/puhome/23063003r/refgame_project/models/gpt2" # æœ¬åœ° GPT-2 æ¨¡å‹è·¯å¾„
DATA_FILE = "/puhome/23063003r/refgame_project/data/test_data.json" # æ¨¡æ‹Ÿæ•°æ®åº“æ–‡ä»¶
D_HIDDEN = 768 # GPT-2çš„éšè—å±‚ç»´åº¦ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬å…±äº«è¯­ä¹‰ç©ºé—´çš„ç»´åº¦

# --- Agent B (Listener - GPT-2) è§†è§’ ---

# 1. åŠ è½½ GPT-2 æ¨¡å‹å’Œ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = GPT2Model.from_pretrained(MODEL_PATH) # ä½¿ç”¨ GPT2Model è·å–å¥å­åµŒå…¥ï¼Œè€Œä¸æ˜¯ AutoModelForCausalLM
model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

# 2. æ‰©å±• GPT-2 çš„ Tokenizer ä»¥æ”¯æŒä¸­æ–‡
# å‡è®¾æˆ‘ä»¬å·²ç»é¢„å…ˆæ”¶é›†äº†æ‰€æœ‰å¯èƒ½çš„ä¸­æ–‡æ±‰å­—
# å®é™…é¡¹ç›®ä¸­ï¼Œä½ éœ€è¦ä»ä½ çš„ä¸­æ–‡è¯­æ–™ä¸­æå–æ‰€æœ‰ä¸é‡å¤çš„æ±‰å­—å¹¶æ·»åŠ åˆ°è¿™é‡Œ
all_chinese_chars_in_corpus = set("ä¸€ä¸ªè‹¹æœæ‰åˆ°äº†åœ°ä¸Šã€‚çŒ«è·³åˆ°äº†æ¡Œå­ä¸Šã€‚ä¸€è¾†çº¢è‰²çš„æ±½è½¦å¼€åœ¨è¡—ä¸Šã€‚") # ç¤ºä¾‹æ±‰å­—
tokenizer.add_special_tokens({'additional_special_tokens': list(all_chinese_chars_in_corpus)})
model.resize_token_embeddings(len(tokenizer)) # è°ƒæ•´ embedding å±‚å¤§å°ä»¥é€‚åº”æ–°è¯æ±‡è¡¨

print(f"âœ… GPT-2 tokenizer å·²æ‰©å±•ï¼Œæ–°çš„è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
print(f"âœ… GPT-2 æ¨¡å‹ Embedding å±‚å·²è°ƒæ•´ã€‚")

# 3. å®šä¹‰æŠ•å½±å±‚ (å°†GPT-2çš„è¾“å‡ºæ˜ å°„åˆ°å…±äº«è¯­ä¹‰ç©ºé—´)
# å…±äº«è¯­ä¹‰ç©ºé—´ç»´åº¦ä¸GPT-2çš„éšè—å±‚ç»´åº¦ç›¸åŒï¼Œå› æ­¤è¿™é‡Œå¯ä»¥ç†è§£ä¸ºæ’ç­‰æ˜ å°„æˆ–å¾®è°ƒã€‚
# ä¹Ÿå¯ä»¥æ˜¾å¼å®šä¹‰ä¸€ä¸ªçº¿æ€§å±‚ï¼štorch.nn.Linear(model.config.hidden_size, D_HIDDEN)
# ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨GPT-2çš„è¾“å‡ºä½œä¸ºå…±äº«è¯­ä¹‰å‘é‡ã€‚
# å¦‚æœéšè—å±‚å¤§å°å’Œå…±äº«ç©ºé—´ç»´åº¦ä¸åŒï¼Œåˆ™éœ€è¦æ­¤å±‚ã€‚
# è¿™é‡Œ D_HIDDEN åº”è¯¥ä¸ model.config.hidden_size ä¿æŒä¸€è‡´ï¼Œå¯¹äºgpt2æ˜¯768
assert D_HIDDEN == model.config.hidden_size, "D_HIDDEN must match GPT-2's hidden_size for direct use."

# 4. ä¼˜åŒ–å™¨
optimizer = AdamW(model.parameters(), lr=1e-5)

# --- æ¨¡æ‹Ÿä¸€è½®æ¸¸æˆ ---

# 5. ä»æ¨¡æ‹Ÿæ•°æ®åº“åŠ è½½ä¸€è½®æ¸¸æˆæ•°æ®
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        game_data_list = json.load(f)
    game_round = game_data_list[0] # å–ç¬¬ä¸€è½®æ¸¸æˆæ•°æ®
except FileNotFoundError:
    print(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ '{DATA_FILE}' æœªæ‰¾åˆ°ã€‚è¯·åˆ›å»ºå®ƒã€‚")
    exit()
except Exception as e:
    print(f"âŒ é”™è¯¯åŠ è½½æ•°æ®: {e}")
    exit()

print("\n--- æ¨¡æ‹Ÿæ¸¸æˆå¼€å§‹ ---")
print(f"ğŸ¯ ç›®æ ‡ä¸­æ–‡å¥å­ (CPM 'è¯´'): {game_round['target_sentence_chinese_raw']}")
print(f"ğŸ“š å€™é€‰è‹±æ–‡å¥å­ (Agent B é€‰æ‹©): {game_round['candidate_english_sentences_raw']}")
print(f"âœ… æ­£ç¡®ç´¢å¼•: {game_round['correct_candidate_index']}")

# 6. Agent A (CPM è§†è§’) 'è¯´' (æä¾›ä¸­æ–‡å¥å­ä½œä¸ºä¹±ç æº)
# æˆ‘ä»¬æ‰®æ¼”CPMï¼Œæä¾›ç›®æ ‡ä¸­æ–‡å¥å­ã€‚GPT-2å°†æŠŠå®ƒè§†ä¸ºä¹±ç ã€‚
cpm_spoken_chinese_sentence = game_round['target_sentence_chinese_raw']

# 7. Agent B (GPT-2) å¤„ç†ä¸­æ–‡ä¹±ç è¾“å…¥
# æ‹¿åˆ° embedding å±‚ï¼Œç”¨äºæ¯”è¾ƒå˜åŒ–
embedding_before = model.wte.weight.clone().detach() # wte æ˜¯ Word Token Embeddings

# å°†ä¸­æ–‡å¥å­æ‹†åˆ†ä¸ºå­—ç¬¦ï¼Œå¹¶ç”¨ GPT-2 çš„ tokenizer å¤„ç†
# tokenizerä¼šè‡ªåŠ¨è¯†åˆ«æ¯ä¸ªæ±‰å­—ä¸ºæ·»åŠ çš„ç‰¹æ®Štoken
inputs_cn_symbolic = tokenizer(cpm_spoken_chinese_sentence, return_tensors="pt")

# è·å–ä¸­æ–‡ä¹±ç åºåˆ—çš„è¯­ä¹‰è¡¨ç¤º
# ä½¿ç”¨ model(input_ids).last_hidden_state æ¥è·å– Encoder è¾“å‡º
outputs_cn_symbolic = model(**inputs_cn_symbolic)
# è¿™é‡Œæˆ‘ä»¬å–åºåˆ—çš„ç¬¬ä¸€ä¸ªtokençš„éšè—çŠ¶æ€ä½œä¸ºå¥å­è¡¨ç¤º
# (å¯¹äºå¥å­çº§åˆ«çš„ä»»åŠ¡ï¼Œé€šå¸¸ä¼šè¿™ä¹ˆåšï¼Œæˆ–è€…è¿›è¡Œå¹³å‡æ± åŒ–)
semantic_vector_B_from_A = outputs_cn_symbolic.last_hidden_state[:, 0, :]
# æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰é¢å¤–çš„æŠ•å½±å±‚ï¼Œå› ä¸º D_HIDDEN == model.config.hidden_size

print(f"\n Agent B æ¥æ”¶åˆ°ä¸­æ–‡ä¹±ç åºåˆ—ï¼Œå°†å…¶ç¼–ç ä¸ºè¯­ä¹‰å‘é‡ (å½¢çŠ¶: {semantic_vector_B_from_A.shape})")

# 8. Agent B å¤„ç†è‹±æ–‡å€™é€‰å¥å­
# å¯¹æ¯ä¸ªè‹±æ–‡å€™é€‰å¥å­è¿›è¡Œç¼–ç ï¼Œå¹¶è·å–å…¶è¯­ä¹‰è¡¨ç¤º
semantic_vectors_B_candidates = []
for eng_sentence in game_round['candidate_english_sentences_raw']:
    inputs_en = tokenizer(eng_sentence, return_tensors="pt")
    outputs_en = model(**inputs_en)
    vec_en = outputs_en.last_hidden_state[:, 0, :]
    semantic_vectors_B_candidates.append(vec_en)

semantic_vectors_B_candidates = torch.cat(semantic_vectors_B_candidates, dim=0)
print(f" Agent B å°†è‹±æ–‡å€™é€‰å¥å­ç¼–ç ä¸ºè¯­ä¹‰å‘é‡ (å½¢çŠ¶: {semantic_vectors_B_candidates.shape})")


# 9. Agent B çŒœæµ‹ (è®¡ç®—ç›¸ä¼¼åº¦å¹¶é¢„æµ‹)
# semantic_vector_B_from_A: (1, D_HIDDEN)
# semantic_vectors_B_candidates: (num_candidates, D_HIDDEN)
similarities = F.cosine_similarity(semantic_vector_B_from_A, semantic_vectors_B_candidates, dim=1)
predicted_index = torch.argmax(similarities).item()

print(f"\n ç›¸ä¼¼åº¦å¾—åˆ† (è¶Šé«˜è¶Šç›¸ä¼¼): {similarities.tolist()}")
print(f" Agent B çŒœæµ‹çš„ç´¢å¼•: {predicted_index}")

# 10. åé¦ˆä¸æƒé‡æ›´æ–° (Agent B å­¦ä¹ )
correct_index_tensor = torch.tensor([game_round['correct_candidate_index']], device=similarities.device)

# ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œå°†ç›¸ä¼¼åº¦è§†ä¸º logits
# æ³¨æ„ï¼šCrossEntropyLossæœŸæœ›çš„è¾“å…¥æ˜¯æœªç»å½’ä¸€åŒ–çš„logitsï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥ç”¨cosine_similarityä½œä¸ºlogits
# å¦‚æœç›¸ä¼¼åº¦å€¼åŸŸä¸æ˜¯0åˆ°1ï¼Œæˆ–è€…ä½ éœ€è¦æ›´ä¸¥æ ¼çš„åˆ†ç±»ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æˆ–æ·»åŠ çº¿æ€§å±‚ã€‚
# ä½†å¯¹äºç®€å•çš„æ’åä»»åŠ¡ï¼Œç›´æ¥ç”¨ç›¸ä¼¼åº¦ä½œä¸ºâ€œå¾—åˆ†â€å¹¶è®¡ç®—äº¤å‰ç†µæ˜¯å¯è¡Œçš„ã€‚
loss = F.cross_entropy(similarities.unsqueeze(0), correct_index_tensor)

optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦
loss.backward()      # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
optimizer.step()     # æ›´æ–°æ¨¡å‹æƒé‡

# 11. æ¯”è¾ƒ Embedding å˜åŒ–
embedding_after = model.wte.weight.detach()
diff = torch.norm(embedding_after - embedding_before).item()

print(f"\n æƒé‡æ›´æ–°å®Œæˆã€‚")
print(f" æœ¬è½®æ¸¸æˆæŸå¤±: {loss.item():.4f}")
print(f" Embedding (word token embeddings) æ”¹å˜é‡: {diff:.6f}")
print(f" Agent B çŒœå¯¹äº†å—ï¼Ÿ: {predicted_index == game_round['correct_candidate_index']}")
