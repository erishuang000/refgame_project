import torch
from transformers import AutoTokenizer, GPT2Model
import json
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
import os
import random # å¼•å…¥randomç”¨äºshuffleæ•°æ®é›†

# --- 1. é…ç½®ç®¡ç† ---
class Config:
    MODEL_PATH = "/puhome/23063003r/refgame_project/models/gpt2"
    DATA_FILE = "/hpc2/puhome/23063003r/refgame_project/data/generated_game_data.json"
    OUTPUT_DIR = "/puhome/23063003r/refgame_project/output/"
    D_HIDDEN = 768  # GPT-2çš„éšè—å±‚ç»´åº¦

    # Loss å¥–åŠ±/æƒ©ç½šç³»æ•°
    REWARD_CORRECT = 0.1
    PENALTY_WRONG = 1.0

    LEARNING_RATE = 1e-5
    BATCH_SIZE = 1

# --- Listener Loss å‡½æ•° ---
def listener_mse_reciprocal_loss(
    semantic_vector_from_agent_A: torch.Tensor,
    semantic_vectors_candidates_B: torch.Tensor,
    correct_candidate_index: torch.Tensor, # ç¡®ä¿è¿™é‡Œæ˜¯ LongTensor
    epsilon: float = 1e-8
) -> torch.Tensor:
    """
    è®¡ç®— Listener Lossï¼Œé‡‡ç”¨è®ºæ–‡ä¸­ (EMERGENT TRANSLATION IN MULTI-AGENT COMMUNICATION)
    æè¿°çš„ MSE å€’æ•°å¯¹æ•°å½¢å¼ã€‚
    Args:
        semantic_vector_from_agent_A (torch.Tensor): Agent Aï¼ˆä¸­æ–‡ä¹±ç ï¼‰çš„è¯­ä¹‰å‘é‡ã€‚å½¢çŠ¶: (batch_size, D_HIDDEN)
        semantic_vectors_candidates_B (torch.Tensor): Agent B å€™é€‰è‹±æ–‡å¥å­çš„è¯­ä¹‰å‘é‡é›†åˆã€‚å½¢çŠ¶: (batch_size, num_candidates, D_HIDDEN)
        correct_candidate_index (torch.Tensor): æ­£ç¡®å€™é€‰å¥å­çš„ç´¢å¼•ã€‚å½¢çŠ¶: (batch_size,)
        epsilon (float): ç”¨äºæ•°å€¼ç¨³å®šçš„å°å¸¸æ•°ã€‚
    Returns:
        torch.Tensor: è®¡ç®—å‡ºçš„æŸå¤±å€¼ã€‚
    """
    print(f"DEBUG: listener_mse_reciprocal_loss input shapes & dtypes:")
    print(f"  semantic_vector_from_agent_A: {semantic_vector_from_agent_A.shape}, {semantic_vector_from_agent_A.dtype}")
    print(f"  semantic_vectors_candidates_B: {semantic_vectors_candidates_B.shape}, {semantic_vectors_candidates_B.dtype}")
    print(f"  correct_candidate_index: {correct_candidate_index.shape}, {correct_candidate_index.dtype}") # ç¡®ä¿æ˜¯ torch.long

    expanded_vector_A = semantic_vector_from_agent_A.unsqueeze(1)
    print(f"DEBUG: expanded_vector_A shape: {expanded_vector_A.shape}")

    squared_diff = (expanded_vector_A - semantic_vectors_candidates_B).pow(2)
    print(f"DEBUG: squared_diff shape: {squared_diff.shape}")

    mse_distances = squared_diff.sum(dim=-1)
    print(f"DEBUG: mse_distances shape: {mse_distances.shape}")

    logits = 1 / (mse_distances + epsilon)
    print(f"DEBUG: logits shape: {logits.shape}")

    loss = F.cross_entropy(logits, correct_candidate_index) # correct_candidate_index å¿…é¡»æ˜¯ torch.long

    return loss

# --- Agent B (Listener) æ¨¡å‹ç±» ---
class AgentBListener(torch.nn.Module):
    def __init__(self, model_path, all_chinese_chars, hidden_dim):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = GPT2Model.from_pretrained(model_path)

        self.tokenizer.add_special_tokens({'additional_special_tokens': list(all_chinese_chars)})
        self.model.resize_token_embeddings(len(self.tokenizer))

        assert hidden_dim == self.model.config.hidden_size, \
            "D_HIDDEN must match GPT-2's hidden_size for direct use as semantic vector."

        print(f"âœ… AgentB: GPT-2 tokenizer å·²æ‰©å±•ï¼Œæ–°çš„è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        print(f"âœ… AgentB: GPT-2 æ¨¡å‹ Embedding å±‚å·²è°ƒæ•´ã€‚")

    def forward(self, inputs_cn_raw, inputs_en_candidates_list_raw, device):
        # inputs_cn_raw: å½¢çŠ¶ (batch_size,) çš„å­—ç¬¦ä¸²å…ƒç»„/åˆ—è¡¨ (e.g., ['ä½ çœ‹èµ·æ¥åƒä¸€ä¸ªèªæ˜äººã€‚'])
        # inputs_en_candidates_list_raw: å½¢çŠ¶ (batch_size, num_candidates) çš„å…ƒç»„çš„å…ƒç»„ (e.g., (('Tom', 'Sami', 'You'),))

        # è®°å½•æ¨¡å‹æƒé‡åœ¨è®¡ç®—å‰çš„çŠ¶æ€
        embedding_before = self.model.wte.weight.clone().detach()

        # å¤„ç†ä¸­æ–‡ä¹±ç è¾“å…¥ (æ‰¹å¤„ç†)
        inputs_cn_symbolic = self.tokenizer(
            list(inputs_cn_raw), # DataLoader å¯èƒ½è¿”å›å…ƒç»„ï¼Œè½¬æˆåˆ—è¡¨ä»¥ä¾¿tokenizerå¤„ç†
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(device)

        outputs_cn_symbolic = self.model(**inputs_cn_symbolic)
        # å– CLS token æˆ–å¹³å‡æ± åŒ–ï¼Œè¿™é‡Œå–ç¬¬ä¸€ä¸ªtokençš„éšè—çŠ¶æ€ä½œä¸ºå¥å­è¡¨ç¤º
        semantic_vector_B_from_A = outputs_cn_symbolic.last_hidden_state[:, 0, :]

        print(f"DEBUG_FORWARD: semantic_vector_B_from_A shape: {semantic_vector_B_from_A.shape}")

        # å¤„ç†è‹±æ–‡å€™é€‰å¥å­ (æ‰¹å¤„ç†)
        # inputs_en_candidates_list_raw å¯èƒ½æ˜¯ (('å€™é€‰1', 'å€™é€‰2', 'å€™é€‰3'),)
        # éœ€æå–å‡º ['å€™é€‰1', 'å€™é€‰2', 'å€™é€‰3']

        # å±•å¹³å€™é€‰åˆ—è¡¨ï¼Œå› ä¸ºtokenizeræœŸæœ›çš„æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¹æ¬¡æ‰€æœ‰å€™é€‰çš„å•å±‚åˆ—è¡¨
        # ä¾‹å¦‚ï¼Œå¦‚æœ batch_size=2ï¼Œnum_candidates=3ï¼Œè¾“å…¥å¯èƒ½æ˜¯ (('A1','A2','A3'), ('B1','B2','B3'))
        # å±•å¹³åä¸º ['A1','A2','A3','B1','B2','B3']
        flat_en_candidates = [sentence for sublist in inputs_en_candidates_list_raw for sentence in sublist]

        inputs_en_candidates_tokenized = self.tokenizer(
            flat_en_candidates,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(device)

        print(f"DEBUG_FORWARD: inputs_en_candidates_tokenized input_ids shape: {inputs_en_candidates_tokenized['input_ids'].shape}")

        outputs_en_candidates = self.model(**inputs_en_candidates_tokenized)

        # æå–æ¯ä¸ªå€™é€‰å¥å­çš„è¯­ä¹‰å‘é‡ï¼Œå½¢çŠ¶æ˜¯ (total_candidates_in_batch, D_HIDDEN)
        flat_semantic_vectors_B_candidates = outputs_en_candidates.last_hidden_state[:, 0, :]

        # å°†å±•å¹³çš„è¯­ä¹‰å‘é‡é‡æ–°å¡‘å½¢ä¸º (batch_size, num_candidates, D_HIDDEN)
        # num_candidates = len(inputs_en_candidates_list_raw[0]) # ä»ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å€™é€‰æ•°è·å–
        num_candidates = len(inputs_en_candidates_list_raw[0])
        print(f"DEBUG_FORWARD: Detected num_candidates per sample: {num_candidates}")

        semantic_vectors_B_candidates = flat_semantic_vectors_B_candidates.view(
            -1, num_candidates, self.model.config.hidden_size # -1 for batch_size
        )
        print(f"DEBUG_FORWARD: semantic_vectors_B_candidates shape (after reshape): {semantic_vectors_B_candidates.shape}")

        return semantic_vector_B_from_A, semantic_vectors_B_candidates, embedding_before

    def get_embedding_after(self):
        return self.model.wte.weight.detach()

# --- æ¸¸æˆæ•°æ®åŠ è½½å™¨ ---
class GameDataset(Dataset):
    def __init__(self, data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # ç›´æ¥è¿”å›åŸå§‹å­—å…¸
        return self.data[idx]

# --- 5. æ¸¸æˆè®­ç»ƒå¾ªç¯ ---
class GameTrainer:
    def __init__(self, config: Config, model: AgentBListener, optimizer: AdamW, device: torch.device):
        self.config = config
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.per_round_metrics = []
        self.total_loss_sum = 0.0
        self.correct_predictions_count = 0

    def train_one_round(self, game_round: dict, round_idx: int, total_rounds: int):
        self.optimizer.zero_grad()

        # ä» DataLoader å¾—åˆ°çš„ game_round å·²ç»æ˜¯ PyTorch Tensor æˆ– List/Tuple çš„æ‰¹æ¬¡å½¢å¼
        # è¿™é‡Œä¸å†è¿›è¡Œè§£åŒ…ï¼Œè€Œæ˜¯ç›´æ¥ä¼ é€’ç»™ AgentBListener.forward
        # å› ä¸º batch_size=1ï¼Œæ‰€ä»¥è¿™äº›æ‰¹æ¬¡ç»´åº¦æ˜¯ 1
        cpm_spoken_chinese_sentence_batch = game_round['target_sentence_chinese_raw']
        candidate_english_sentences_batch = game_round['candidate_english_sentences_raw']
        correct_candidate_index_batch = game_round['correct_candidate_index'] # å·²ç»æ˜¯ Tensor

        # 1. Agent B (GPT-2) å¤„ç†ä¸­æ–‡ä¹±ç è¾“å…¥åŠè‹±æ–‡å€™é€‰å¥å­
        semantic_vector_B_from_A, semantic_vectors_B_candidates, embedding_before = \
            self.model(
                cpm_spoken_chinese_sentence_batch, # ç›´æ¥ä¼ å…¥ DataLoader è¿”å›çš„æ‰¹æ¬¡
                candidate_english_sentences_batch, # ç›´æ¥ä¼ å…¥ DataLoader è¿”å›çš„æ‰¹æ¬¡
                self.device
            )

        # 2. Agent B çŒœæµ‹ (è®¡ç®—ç›¸ä¼¼åº¦å¹¶é¢„æµ‹)
        # similarities: (batch_size, num_candidates)
        similarities = F.cosine_similarity(
            semantic_vector_B_from_A.unsqueeze(1), # (batch_size, 1, D_HIDDEN)
            semantic_vectors_B_candidates,         # (batch_size, num_candidates, D_HIDDEN)
            dim=2 # æ²¿ç€ D_HIDDEN ç»´åº¦è®¡ç®—ç›¸ä¼¼åº¦
        ).squeeze(1) # ç»“æœå½¢çŠ¶ (batch_size, num_candidates)

        # å¯¹äº batch_size=1ï¼Œpredicted_index ä¾ç„¶å–ç¬¬ä¸€ä¸ª
        predicted_index = torch.argmax(similarities[0]).item()

        print(f"ğŸ¤” ç›¸ä¼¼åº¦å¾—åˆ† (è¶Šé«˜è¶Šç›¸ä¼¼): {similarities[0].tolist()}")
        print(f"ğŸ”® Agent B çŒœæµ‹çš„ç´¢å¼•: {predicted_index}")

        # 3. åé¦ˆä¸æƒé‡æ›´æ–° (Agent B å­¦ä¹ )
        correct_index_tensor_batch = correct_candidate_index_batch.to(self.device) # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š

        # ä½¿ç”¨ Listener Loss å‡½æ•°
        base_loss = listener_mse_reciprocal_loss(
            semantic_vector_B_from_A,
            semantic_vectors_B_candidates,
            correct_index_tensor_batch # ä¼ å…¥ Tensor
        )

        is_correct = (predicted_index == correct_index_tensor_batch.item()) # å¯¹äºbatch_size=1ï¼Œæ¯”è¾ƒå•ä¸ªå€¼
        if is_correct:
            loss = base_loss * (1 - self.config.REWARD_CORRECT)
            outcome_message = f"ğŸ‰ Agent B çŒœå¯¹å•¦ï¼æŸå¤±è°ƒæ•´ç³»æ•°: {(1 - self.config.REWARD_CORRECT):.2f}"
        else:
            loss = base_loss * self.config.PENALTY_WRONG
            outcome_message = f"ğŸ’” Agent B çŒœé”™äº†ï¼æŸå¤±è°ƒæ•´ç³»æ•°: {self.config.PENALTY_WRONG:.2f}"

        print(outcome_message)

        loss.backward()
        self.optimizer.step()

        # 4. æ¯”è¾ƒ Embedding å˜åŒ–
        embedding_after = self.model.get_embedding_after()
        diff = torch.norm(embedding_after - embedding_before).item()

        self.total_loss_sum += loss.item()
        if is_correct:
            self.correct_predictions_count += 1

        print(f"ğŸ“‰ æœ¬è½®æ¸¸æˆæœ€ç»ˆæŸå¤±: {loss.item():.4f}")
        print(f"ğŸ” Embedding (word token embeddings) æ”¹å˜é‡: {diff:.6f}")
        print(f"âœ¨ Agent B æœ€ç»ˆçŒœæµ‹ç»“æœ: {is_correct}")

        # è®°å½•æœ¬è½®æ•°æ® (æ³¨æ„è§£åŒ…åŸå§‹å­—ç¬¦ä¸²ä»¥ä¾¿ä¿å­˜JSON)
        round_data = {
            "round_idx": round_idx,
            "chinese_sentence": game_round['target_sentence_chinese_raw'][0], # è§£åŒ…
            "correct_english_sentence": game_round['correct_english_sentence_raw'][0], # è§£åŒ…
            "candidate_english_sentences": game_round['candidate_english_sentences_raw'][0], # è§£åŒ…
            "correct_candidate_idx": game_round['correct_candidate_index'][0].item(), # è§£åŒ…å¹¶è½¬ä¸ºint
            "predicted_index": predicted_index,
            "similarities": similarities[0].tolist(), # è§£åŒ…å¹¶è½¬ä¸ºlist
            "is_correct_prediction": is_correct,
            "base_loss": base_loss.item(),
            "final_loss": loss.item(),
            "embedding_diff_norm": diff
        }
        self.per_round_metrics.append(round_data)

    def train(self):
        game_dataset = GameDataset(self.config.DATA_FILE)
        # ä½¿ç”¨ collate_fn=self._custom_collate_fn ç¡®ä¿æ•°æ®ä»¥é¢„æœŸæ ¼å¼è¿›å…¥
        # æ³¨æ„: å³ä½¿ batch_size=1, collate_fn ä¹Ÿä¼šè¢«è°ƒç”¨
        data_loader = DataLoader(game_dataset, batch_size=self.config.BATCH_SIZE,
                                 shuffle=False, collate_fn=self._custom_collate_fn)

        self.model.model.train() # Set model to training mode
        total_rounds = len(data_loader)
        print(f"\n--- å‡†å¤‡è¿›è¡Œ {total_rounds} è½®æ¸¸æˆ ---")

        for i, game_round_batch in enumerate(data_loader):
            print(f"\n--- æ¸¸æˆå›åˆ {i + 1}/{total_rounds} ---")

            # ç”±äºæˆ‘ä»¬æœ‰è‡ªå®šä¹‰çš„ collate_fnï¼Œgame_round_batch å·²ç»æ˜¯æ‰¹å¤„ç†åçš„ Tensor æˆ– List
            # ä½†ä¸ºäº†æ‰“å°æ—¥å¿—ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä»æ‰¹æ¬¡ä¸­å–å‡ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ•°æ®
            # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥æ‰“å° batch_data çš„ [0] å¯èƒ½ä¼šå› ç±»å‹ä¸åŒè€Œå¤±è´¥
            # æœ€å¥½ä»å®é™…ä¼ å…¥ train_one_round çš„å‚æ•°ä¸­è·å–
            print(f"ğŸ¯ ç›®æ ‡ä¸­æ–‡å¥å­ (CPM 'è¯´'): {game_round_batch['target_sentence_chinese_raw'][0]}")
            print(f"ğŸ“š å€™é€‰è‹±æ–‡å¥å­ (Agent B é€‰æ‹©): {game_round_batch['candidate_english_sentences_raw'][0]}")
            print(f"âœ… æ­£ç¡®ç´¢å¼•: {game_round_batch['correct_candidate_index'][0].item()}")

            self.train_one_round(game_round_batch, i + 1, total_rounds) # ç›´æ¥ä¼ å…¥æ‰¹æ¬¡æ•°æ®

        # --- è®­ç»ƒç»“æŸï¼Œæ±‡æ€»ç»“æœå¹¶ä¿å­˜ ---
        print("\n--- è®­ç»ƒæ€»ç»“ ---")
        final_accuracy_percentage = (self.correct_predictions_count / total_rounds * 100) if total_rounds > 0 else 0
        print(f"æ€»è½®æ•°: {total_rounds}")
        print(f"å¹³å‡æŸå¤±: {self.total_loss_sum / total_rounds:.4f}")
        print(f"çŒœå¯¹è½®æ•°: {self.correct_predictions_count}")
        print(f"å‡†ç¡®ç‡: {final_accuracy_percentage:.2f}%")

        # --- ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶ ---
        summary_metrics = {
            "total_rounds": total_rounds,
            "final_average_loss": self.total_loss_sum / total_rounds,
            "final_correct_count": self.correct_predictions_count,
            "final_accuracy_percentage": final_accuracy_percentage
        }

        output_data = {
            "summary_metrics": summary_metrics,
            "per_round_metrics": self.per_round_metrics
        }

        if not os.path.exists(self.config.OUTPUT_DIR):
            os.makedirs(self.config.OUTPUT_DIR)

        output_file_path = os.path.join(self.config.OUTPUT_DIR, "training_results.json")
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ‰ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")

    # è‡ªå®šä¹‰ collate_fn
    def _custom_collate_fn(self, batch_list_of_dicts):
        # batch_list_of_dicts æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ GameDataset.__getitem__ è¿”å›çš„ä¸€ä¸ªå­—å…¸
        # ä¾‹å¦‚ï¼š[{'target_cn': 'ä½ å¥½', 'candidates_en': ['hi', 'hello']}, ...]

        collated_batch = {}
        # åˆå§‹åŒ–åˆ—è¡¨æ¥æ”¶é›†æ‰€æœ‰å­—æ®µ
        chinese_sentences = []
        correct_english_sentences = []
        candidate_english_sentences_list = [] # è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ (batch_size, num_candidates)
        correct_candidate_indices = []

        for item in batch_list_of_dicts:
            chinese_sentences.append(item['target_sentence_chinese_raw'])
            correct_english_sentences.append(item['correct_english_sentence_raw'])
            candidate_english_sentences_list.append(item['candidate_english_sentences_raw'])
            correct_candidate_indices.append(item['correct_candidate_index'])

        collated_batch['target_sentence_chinese_raw'] = chinese_sentences # list of strings (batch_size,)
        collated_batch['correct_english_sentence_raw'] = correct_english_sentences # list of strings (batch_size,)
        collated_batch['candidate_english_sentences_raw'] = candidate_english_sentences_list # list of lists (batch_size, num_candidates)
        collated_batch['correct_candidate_index'] = torch.tensor(correct_candidate_indices, dtype=torch.long) # Tensor (batch_size,)

        return collated_batch


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    all_chinese_chars_in_corpus = set()
    try:
        with open(Config.DATA_FILE, 'r', encoding='utf-8') as f:
            temp_game_data = json.load(f)
        for entry in temp_game_data:
            all_chinese_chars_in_corpus.update(list(entry['target_sentence_chinese_raw']))
    except Exception as e:
        print(f"âŒ é”™è¯¯åŠ è½½æ•°æ®ä»¥æ”¶é›†ä¸­æ–‡ç¬¦å·ï¼Œä½¿ç”¨é»˜è®¤é›†: {e}")
        all_chinese_chars_in_corpus = set("ä¸€ä¸ªè‹¹æœæ‰åˆ°äº†åœ°ä¸Šã€‚çŒ«è·³åˆ°äº†æ¡Œå­ä¸Šã€‚ä¸€è¾†çº¢è‰²çš„æ±½è½¦å¼€åœ¨è¡—ä¸Šã€‚ç‹—è¿½çƒã€‚å¤©ç©ºæ˜¯è“è‰²çš„ã€‚å¥¹åœ¨çœ‹ä¹¦ã€‚ç¡æ²™å‘ã€‚å­©å­ä»¬åœ¨å…¬å›­ç©ã€‚å¤ªé˜³ä»ä¸œæ–¹å‡èµ·ã€‚å–œæ¬¢å¬éŸ³ä¹ã€‚å’–å•¡å¾ˆçƒ«ã€‚æˆ‘é¥¿äº†æƒ³åƒä¸œè¥¿ã€‚")


    agent_b_model = AgentBListener(Config.MODEL_PATH, all_chinese_chars_in_corpus, Config.D_HIDDEN)
    agent_b_model.to(device)

    optimizer = AdamW(agent_b_model.parameters(), lr=Config.LEARNING_RATE)

    trainer = GameTrainer(Config, agent_b_model, optimizer, device)

    trainer.train()
