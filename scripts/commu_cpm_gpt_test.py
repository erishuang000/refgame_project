import torch
from transformers import AutoTokenizer, GPT2Model, AutoModelWithLMHead, GPT2Config, T5Config, AutoModelForCausalLM
import json
import torch.nn.functional as F
from torch.optim import AdamW
import os
import random

# --- 1. å…¨å±€é…ç½® ---
# æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹è·¯å¾„
GPT2_MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/gpt2"
CPM_MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/CPM-Generate"
DATA_FILE = "/ubsnhome/23063003r/refgame_project/data/bilingual_game_data.json" # éœ€è¦æ–°çš„æ•°æ®æ ¼å¼
OUTPUT_DIR = "/ubsnhome/23063003r/refgame_project/output/"

# å­¦ä¹ å‚æ•°
LEARNING_RATE_GPT2 = 5e-6
LEARNING_RATE_CPM = 2e-6
REWARD_CORRECT = 0.1 # çŒœå¯¹æ—¶, æŸå¤±é™ä½ 10%
PENALTY_WRONG = 1.1  # çŒœé”™æ—¶, æŸå¤±å¢åŠ  10%
ALIGNMENT_LOSS_WEIGHT = 0.5 # å¯¹é½æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡

# åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# --- 2. Agent å°è£… ã€å…³é”®è®¾è®¡ã€‘---
# ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ç±»æ¥ç®¡ç†æ¯ä¸ª Agent çš„èµ„äº§ï¼Œä½¿ä»£ç æ›´æ¸…æ™°
class Agent:
    def __init__(self, model_name, model, tokenizer, optimizer):
        self.name = model_name
        self.model = model
        self.tokenizer = tokenizer
        self.optimizer = optimizer
        self.error_log = [] # ç”¨äºè®°å½•â€œæˆ‘è¯´çš„è¯ -> å¯¹æ–¹çŒœé”™äº†â€

    def get_semantic_embedding(self, text):
        """è·å–å•ä¸ªå¥å­çš„è¯­ä¹‰åµŒå…¥ (å–[CLS]æˆ–ç¬¬ä¸€ä¸ªtokençš„è¾“å‡º)"""
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        outputs = self.model(**inputs)
        # å…¼å®¹ GPT2Model å’Œ CPM-Generate çš„è¾“å‡ºç»“æ„
        if hasattr(outputs, 'last_hidden_state'):
            embedding = outputs.last_hidden_state[:, 0, :]
        else: # é€‚é… CPM-Generate (å¯èƒ½åœ¨ model.transformer.h ä¸­)
            embedding = outputs.hidden_states[-1][:, 0, :]
        return embedding

    def get_candidate_embeddings(self, sentences):
        """è·å–å¤šä¸ªå€™é€‰å¥å­çš„åµŒå…¥"""
        # æ³¨æ„ï¼šä¸ºäº†è·å¾—å¯æ¯”è¾ƒçš„åµŒå…¥ï¼Œæœ€å¥½é€ä¸ªå¤„ç†å¥å­è€Œä¸æ˜¯æ‰¹å¤„ç†åå¹³å‡
        # å› ä¸ºæ¯ä¸ªå¥å­çš„é•¿åº¦å’ŒtokenåŒ–ç»“æœä¸åŒ
        embeddings = [self.get_semantic_embedding(s) for s in sentences]
        return torch.cat(embeddings, dim=0)

# --- 3. åˆå§‹åŒ–ä¸¤ä¸ª Agent ---

# ç¡®å®šè®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åŠ è½½ GPT-2
print("--- åˆå§‹åŒ–è‹±æ–‡ Agent (GPT-2) ---")
gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_PATH)
# GPT-2 çš„åŸç”Ÿ Tokenizer æ²¡æœ‰ pad token
if gpt2_tokenizer.pad_token is None:
    gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
gpt2_model = GPT2Model.from_pretrained(GPT2_MODEL_PATH).to(device)
gpt2_model.resize_token_embeddings(len(gpt2_tokenizer)) # é€‚é… pad token
gpt2_optimizer = AdamW(gpt2_model.parameters(), lr=LEARNING_RATE_GPT2)
agent_gpt2 = Agent("GPT-2", gpt2_model, gpt2_tokenizer, gpt2_optimizer)
agent_gpt2.model.train()

# åŠ è½½ CPM
print("--- åˆå§‹åŒ–ä¸­æ–‡ Agent (CPM) ---")
# AutoModelWithLMHead å·²è¢«å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ AutoModelForCausalLM
cpm_tokenizer = AutoTokenizer.from_pretrained(CPM_MODEL_PATH)
cpm_model = AutoModelForCausalLM.from_pretrained(CPM_MODEL_PATH).to(device)
cpm_optimizer = AdamW(cpm_model.parameters(), lr=LEARNING_RATE_CPM)
# CPM çš„æ ¸å¿ƒæ¨¡å‹åœ¨ .transformer å±æ€§é‡Œ
agent_cpm = Agent("CPM", cpm_model.transformer, cpm_tokenizer, cpm_optimizer)
agent_cpm.model.train() # å°†å†…éƒ¨çš„ transformer è®¾ä¸ºè®­ç»ƒæ¨¡å¼

# ã€é‡è¦ã€‘è®© GPT-2 Tokenizer èƒ½å¤Ÿå¤„ç†ä¸­æ–‡å­—ç¬¦
# è¿™æ˜¯ä½ åŸå§‹ä»£ç ä¸­éå¸¸å¥½çš„ä¸€ä¸ªå®è·µï¼Œæˆ‘ä»¬ä¿ç•™å®ƒ
print("--- æ‰©å±• GPT-2 Tokenizer ä»¥æ”¯æŒä¸­æ–‡ ---")
# å‡è®¾æ•°æ®æ–‡ä»¶ä¸­åŒ…å«äº†æ‰€æœ‰å¯èƒ½å‡ºç°çš„ä¸­æ–‡æ¦‚å¿µ
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        full_game_data = json.load(f)
    all_chinese_concepts = set()
    for entry in full_game_data:
        all_chinese_concepts.add(entry['concept_chinese'])
        all_chinese_concepts.update(entry['distractors_chinese'])

    new_tokens = list(all_chinese_concepts - set(agent_gpt2.tokenizer.get_vocab().keys()))
    agent_gpt2.tokenizer.add_tokens(new_tokens)
    agent_gpt2.model.resize_token_embeddings(len(agent_gpt2.tokenizer))
    print(f"GPT-2 tokenizer å·²æ‰©å±•ï¼Œæ–°è¯æ±‡è¡¨å¤§å°: {len(agent_gpt2.tokenizer)}")
except Exception as e:
    print(f"âŒ åŠ è½½æ•°æ®æˆ–æ‰©å±•Tokenizerå¤±è´¥: {e}")
    print("å°†ä½¿ç”¨é»˜è®¤çš„å°‘é‡ä¸­æ–‡å­—ç¬¦è¿›è¡Œæ‰©å±•ã€‚")
    agent_gpt2.tokenizer.add_tokens(list("é£æœºè€å¸ˆæ©˜å­å­¦æ ¡çŒ«ç‹—å¤ªé˜³ä¹¦"))
    agent_gpt2.model.resize_token_embeddings(len(agent_gpt2.tokenizer))


# --- 4. å‡†å¤‡æ•°æ® ---
# **è¯·æ³¨æ„ï¼š** æ•°æ®æ ¼å¼éœ€è¦è°ƒæ•´ä»¥æ”¯æŒåŒå‘æ¸¸æˆ
# å»ºè®®çš„ `bilingual_game_data.json` æ ¼å¼:
# [
#   {
#     "concept_english": "airplane",
#     "description_english": "It flies in the sky and carries people.",
#     "distractors_english": ["teacher", "orange", "school"],
#     "concept_chinese": "é£æœº",
#     "description_chinese": "å®ƒåœ¨å¤©ä¸Šé£ï¼Œç”¨æ¥è½½äººã€‚",
#     "distractors_chinese": ["è€å¸ˆ", "æ©˜å­", "å­¦æ ¡"]
#   }, ...
# ]
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        all_game_rounds_data = json.load(f)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(all_game_rounds_data)} è½®æ¸¸æˆæ•°æ®ã€‚")
except Exception as e:
    print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½æˆ–è§£ææ•°æ®æ–‡ä»¶ '{DATA_FILE}'. {e}")
    exit()

# --- 5. ä¸»è®­ç»ƒå¾ªç¯ ---
total_rounds = len(all_game_rounds_data)
training_log = []
total_loss_sum = 0.0
correct_predictions_count = 0

print(f"\n--- å¼€å§‹è¿›è¡Œ {total_rounds * 2} è½®äº¤äº’å¼æ¸¸æˆ ---")

for i, game_data in enumerate(all_game_rounds_data):
    # æ¯ä¸ªæ•°æ®ç‚¹å¯ä»¥ç©ä¸¤è½®ï¼Œè§’è‰²äº’æ¢
    for turn in range(2):
        round_idx = i * 2 + turn + 1

        # ã€å…³é”®è®¾è®¡ã€‘è§’è‰²åˆ†é…
        if turn == 0:
            # è½®æ¬¡ 1: GPT-2 æ˜¯ Speaker, CPM æ˜¯ Listener
            speaker_agent = agent_gpt2
            listener_agent = agent_cpm
            description = game_data['description_english']
            correct_option = game_data['concept_chinese']
            distractors = game_data['distractors_chinese']
        else:
            # è½®æ¬¡ 2: CPM æ˜¯ Speaker, GPT-2 æ˜¯ Listener
            speaker_agent = agent_cpm
            listener_agent = agent_gpt2
            description = game_data['description_chinese']
            correct_option = game_data['concept_english']
            distractors = game_data['distractors_english']

        print(f"\n--- æ¸¸æˆå›åˆ {round_idx} ---")
        print(f"ğŸ—£ï¸ Speaker ({speaker_agent.name}) æè¿°: \"{description}\"")

        # å‡†å¤‡é€‰é¡¹
        options = distractors + [correct_option]
        random.shuffle(options) # éšæœºæ‰“ä¹±é€‰é¡¹é¡ºåº
        correct_index = options.index(correct_option)
        print(f"ğŸ‘‚ Listener ({listener_agent.name}) é€‰é¡¹: {options}")
        print(f"âœ… æ­£ç¡®é€‰é¡¹ç´¢å¼•: {correct_index}")

        # --- äº¤äº’ä¸è®¡ç®— ---

        # 1. Listener å¤„ç†æè¿°å’Œé€‰é¡¹ï¼Œå¾—åˆ°åµŒå…¥
        # Listener éœ€è¦å¤„ç†çš„æ˜¯â€œå¤–è¯­â€
        desc_embedding = listener_agent.get_semantic_embedding(description)
        option_embeddings = listener_agent.get_candidate_embeddings(options)

        # 2. Listener è¿›è¡ŒçŒœæµ‹
        similarities = F.cosine_similarity(desc_embedding, option_embeddings, dim=1)
        predicted_index = torch.argmax(similarities).item()

        # --- åé¦ˆä¸å­¦ä¹  ---
        is_correct = (predicted_index == correct_index)
        correct_index_tensor = torch.tensor([correct_index], device=device)

        # 3. è®¡ç®— Listener çš„é€‰æ‹©æŸå¤± (cross_entropy)
        listener_loss = F.cross_entropy(similarities.unsqueeze(0), correct_index_tensor)

        # æ ¹æ®å¯¹é”™è°ƒæ•´ Listener æŸå¤±
        if is_correct:
            final_listener_loss = listener_loss * (1 - REWARD_CORRECT)
            outcome_message = f"ğŸ‰ {listener_agent.name} çŒœå¯¹äº†!"
            correct_predictions_count += 1
        else:
            final_listener_loss = listener_loss * PENALTY_WRONG
            outcome_message = f"ğŸ’” {listener_agent.name} çŒœé”™äº†!"
            # ã€å…³é”®è®¾è®¡ã€‘è®°å½•é”™è¯¯ï¼Œç”¨äº Speaker çš„è‡ªæˆ‘æ”¹è¿›
            speaker_agent.error_log.append({
                "description_given": description,
                "options": options,
                "listener_chose": options[predicted_index],
                "correct_answer": correct_option,
                "round": round_idx
            })

        # 4. ã€å…³é”®è®¾è®¡ã€‘è®¡ç®— Speaker çš„å¯¹é½æŸå¤± (Alignment Loss)
        # ç›®æ ‡ï¼šè®© Speaker äº§ç”Ÿçš„æè¿°çš„åµŒå…¥ï¼Œä¸å…¶â€œæ­£ç¡®ç¿»è¯‘â€çš„åµŒå…¥å¯¹é½
        speaker_desc_embedding = speaker_agent.get_semantic_embedding(description)
        # æ­£ç¡®é€‰é¡¹çš„æ–‡æœ¬ç”± Listener çš„è¯­è¨€å†³å®š
        correct_option_text = correct_option
        # æˆ‘ä»¬ç”¨ Listener Agent æ¥ç¼–ç æ­£ç¡®ç­”æ¡ˆï¼Œä½œä¸ºâ€œç›®æ ‡è¯­ä¹‰â€
        # ä½¿ç”¨ .detach() æ¥é˜»æ­¢æ¢¯åº¦æµå› Listenerï¼Œå› ä¸ºè¿™ä¸ªæŸå¤±æ˜¯ä¸º Speaker è®¾è®¡çš„
        target_semantic_embedding = listener_agent.get_semantic_embedding(correct_option_text).detach()

        # alignment_loss è¡¡é‡ Speaker æè¿°å’Œç›®æ ‡è¯­ä¹‰çš„å·®è·ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·®è·è¶Šå°è¶Šå¥½
        # ä½¿ç”¨è´Ÿçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºæŸå¤±ï¼šç›¸ä¼¼åº¦è¶Šé«˜ï¼ŒæŸå¤±è¶Šå°
        alignment_loss = -F.cosine_similarity(speaker_desc_embedding, target_semantic_embedding).mean()

        # 5. åˆå¹¶æŸå¤±å¹¶åå‘ä¼ æ’­
        # è¿™æ˜¯ä¸¤ä¸ª Agent ååŒå­¦ä¹ çš„æ ¸å¿ƒ
        total_loss = final_listener_loss + ALIGNMENT_LOSS_WEIGHT * alignment_loss
        total_loss_sum += total_loss.item()

        # æ¸…ç©ºä¸¤ä¸ª Agent çš„æ¢¯åº¦
        speaker_agent.optimizer.zero_grad()
        listener_agent.optimizer.zero_grad()

        # åå‘ä¼ æ’­ï¼Œæ¢¯åº¦ä¼šè‡ªåŠ¨æµå‘è®¡ç®—å›¾ä¸­æ¶‰åŠçš„æ‰€æœ‰å‚æ•°
        # å³ speaker_agent.model å’Œ listener_agent.model çš„å‚æ•°éƒ½ä¼šè¢«è®¡ç®—æ¢¯åº¦
        total_loss.backward()

        # åˆ†åˆ«æ›´æ–°ä¸¤ä¸ª Agent çš„æƒé‡
        speaker_agent.optimizer.step()
        listener_agent.optimizer.step()

        # --- æ‰“å°å’Œè®°å½• ---
        print(outcome_message)
        print(f"ğŸ”® ç›¸ä¼¼åº¦: {[f'{s:.3f}' for s in similarities.tolist()]}, é¢„æµ‹ç´¢å¼•: {predicted_index}")
        print(f"ğŸ“‰ æ€»æŸå¤±: {total_loss.item():.4f} (Listener Loss: {final_listener_loss.item():.4f}, Speaker Alignment Loss: {alignment_loss.item():.4f})")

        training_log.append({
            "round_idx": round_idx,
            "speaker": speaker_agent.name,
            "listener": listener_agent.name,
            "description": description,
            "options": options,
            "correct_index": correct_index,
            "predicted_index": predicted_index,
            "is_correct": is_correct,
            "total_loss": total_loss.item()
        })


# --- 6. è®­ç»ƒç»“æŸï¼Œæ±‡æ€»ä¸ä¿å­˜ ---
print("\n--- è®­ç»ƒæ€»ç»“ ---")
total_interactions = len(all_game_rounds_data) * 2
final_accuracy = (correct_predictions_count / total_interactions) * 100 if total_interactions > 0 else 0
print(f"æ€»äº¤äº’è½®æ•°: {total_interactions}")
print(f"æ€»çŒœå¯¹æ¬¡æ•°: {correct_predictions_count}")
print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.2f}%")
print(f"å¹³å‡æŸå¤±: {total_loss_sum / total_interactions:.4f}")

# ä¿å­˜ç»“æœ
output_data = {
    "config": {
        "gpt2_lr": LEARNING_RATE_GPT2,
        "cpm_lr": LEARNING_RATE_CPM,
        "reward": REWARD_CORRECT,
        "penalty": PENALTY_WRONG,
        "alignment_weight": ALIGNMENT_LOSS_WEIGHT
    },
    "summary": {
        "total_interactions": total_interactions,
        "correct_predictions": correct_predictions_count,
        "accuracy": final_accuracy,
        "average_loss": total_loss_sum / total_interactions
    },
    "gpt2_error_log": agent_gpt2.error_log,
    "cpm_error_log": agent_cpm.error_log,
    "detailed_log": training_log
}

output_file_path = os.path.join(OUTPUT_DIR, "interactive_training_results.json")
with open(output_file_path, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"\nğŸ‰ å®Œæ•´è®­ç»ƒæ—¥å¿—å’Œç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")

# å¯ä»¥é€‰æ‹©æ€§åœ°ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
# agent_gpt2.model.save_pretrained(os.path.join(OUTPUT_DIR, "gpt2_finetuned"))
# agent_cpm.model.save_pretrained(os.path.join(OUTPUT_DIR, "cpm_finetuned"))
