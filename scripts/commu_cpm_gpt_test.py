import torch
import torch.nn as nn
from transformers import AutoTokenizer, GPT2Model, AutoModelForCausalLM
import json
import torch.nn.functional as F
from torch.optim import AdamW
import os
import random

# --- 1. å…¨å±€é…ç½® ---
# â—ï¸ è¯·åŠ¡å¿…æ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹ä»¥ä¸‹è·¯å¾„
GPT2_MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/gpt2"
CPM_MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/CPM-Generate"
DATA_FILE = "/ubsnhome/23063003r/refgame_project/data/bilingual_game_data.json" # â—ï¸ ç¡®ä¿ä½¿ç”¨æ”¯æŒåŒå‘æ¸¸æˆçš„æ–°æ•°æ®æ ¼å¼
OUTPUT_DIR = "/ubsnhome/23063003r/refgame_project/output/"

# å­¦ä¹ è¶…å‚æ•°
SHARED_EMBEDDING_DIM = 768      # æŠ•å°„åˆ°çš„å…±äº«ç©ºé—´ç»´åº¦
LEARNING_RATE_GPT2 = 5e-6       # GPT-2 çš„å­¦ä¹ ç‡
LEARNING_RATE_CPM = 2e-6        # CPM çš„å­¦ä¹ ç‡
REWARD_CORRECT = 0.1            # çŒœå¯¹æ—¶, æŸå¤±é™ä½ 10% (ä¹˜ä»¥ 1-0.1=0.9)
PENALTY_WRONG = 1.1             # çŒœé”™æ—¶, æŸå¤±å¢åŠ  10% (ä¹˜ä»¥ 1.1)
ALIGNMENT_LOSS_WEIGHT = 0.5     # å¯¹é½æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡

# --- 2. Agent å°è£…ç±» ---
class Agent:
    """å°è£… Agent çš„æ‰€æœ‰ç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æŠ•å°„å±‚å’Œä¼˜åŒ–å™¨"""
    def __init__(self, model_name, model, tokenizer, learning_rate, device):
        self.name = model_name
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

        # è·å–æ¨¡å‹è‡ªèº«çš„ hidden size
        if hasattr(model.config, 'hidden_size'):
            native_hidden_size = model.config.hidden_size  # For CPM
        elif hasattr(model.config, 'n_embd'):
            native_hidden_size = model.config.n_embd      # For GPT-2
        else:
            raise ValueError(f"Cannot determine hidden size for model {self.name}")

        # åˆ›å»ºä»åŸç”Ÿç»´åº¦åˆ°å…±äº«ç»´åº¦çš„æŠ•å°„å±‚
        self.projection = nn.Linear(native_hidden_size, SHARED_EMBEDDING_DIM).to(self.device)

        # ä¼˜åŒ–å™¨éœ€è¦åŒ…å«æ¨¡å‹å’ŒæŠ•å°„å±‚çš„å‚æ•°
        self.optimizer = AdamW(
            list(self.model.parameters()) + list(self.projection.parameters()),
            lr=learning_rate
        )
        self.error_log = []

    def get_semantic_embedding(self, text):
        """è·å–å•ä¸ªå¥å­çš„è¯­ä¹‰åµŒå…¥ï¼Œå¹¶é€šè¿‡æŠ•å°„å±‚"""
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(self.device)

        # åœ¨æ¨ç†æ¨¡å¼ä¸‹è·å–åŸå§‹åµŒå…¥ï¼Œé¿å…ä¸å¿…è¦çš„æ¢¯åº¦è®¡ç®—
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**inputs)
        self.model.train() # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼

        # å…¼å®¹ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
        if hasattr(outputs, 'last_hidden_state'):
            raw_embedding = outputs.last_hidden_state[:, 0, :]
        else:
            # é€‚ç”¨äºæŸäº›è¿”å›å…ƒç»„è¾“å‡ºçš„æ¨¡å‹
            raw_embedding = outputs[0][:, 0, :]

        # å°†åŸç”ŸåµŒå…¥é€šè¿‡æŠ•å°„å±‚ï¼Œæ­¤æ­¥ä¼šè¿æ¥è®¡ç®—å›¾
        projected_embedding = self.projection(raw_embedding)
        return projected_embedding

    def get_candidate_embeddings(self, sentences):
        """è·å–å¤šä¸ªå€™é€‰å¥å­çš„åµŒå…¥"""
        embeddings = [self.get_semantic_embedding(s) for s in sentences]
        return torch.cat(embeddings, dim=0)

    def train_mode(self):
        """å°†æ¨¡å‹å’ŒæŠ•å°„å±‚éƒ½è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.model.train()
        self.projection.train()

# --- 3. åˆå§‹åŒ–å·¥ä½œ ---
# åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# ç¡®å®šè®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆå§‹åŒ–è‹±æ–‡ Agent (GPT-2)
print("--- åˆå§‹åŒ–è‹±æ–‡ Agent (GPT-2) ---")
gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_PATH)
if gpt2_tokenizer.pad_token is None:
    gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
gpt2_model = GPT2Model.from_pretrained(GPT2_MODEL_PATH).to(device)
gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))
agent_gpt2 = Agent("GPT-2", gpt2_model, gpt2_tokenizer, LEARNING_RATE_GPT2, device)
agent_gpt2.train_mode()

# åˆå§‹åŒ–ä¸­æ–‡ Agent (CPM)
print("--- åˆå§‹åŒ–ä¸­æ–‡ Agent (CPM) ---")
cpm_tokenizer = AutoTokenizer.from_pretrained(CPM_MODEL_PATH)
cpm_model_full = AutoModelForCausalLM.from_pretrained(CPM_MODEL_PATH).to(device)
# æˆ‘ä»¬åªå°† Transformer ä¸»ä½“éƒ¨åˆ†ä½œä¸ºæ¨¡å‹ä¼ å…¥ Agent
agent_cpm = Agent("CPM", cpm_model_full.transformer, cpm_tokenizer, LEARNING_RATE_CPM, device)
agent_cpm.train_mode()


# --- 4. å‡†å¤‡æ•°æ® ---
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        all_game_rounds_data = json.load(f)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(all_game_rounds_data)} è½®æ¸¸æˆæ•°æ®ã€‚")
except Exception as e:
    print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½æˆ–è§£ææ•°æ®æ–‡ä»¶ '{DATA_FILE}'. è¯·æ£€æŸ¥è·¯å¾„å’ŒJSONæ ¼å¼ã€‚ {e}")
    exit()

# --- 5. ä¸»è®­ç»ƒå¾ªç¯ ---
total_rounds = len(all_game_rounds_data)
training_log = []
total_loss_sum = 0.0
correct_predictions_count = 0

print(f"\n--- å¼€å§‹è¿›è¡Œ {total_rounds * 2} è½®äº¤äº’å¼æ¸¸æˆ ---")

# æ¯ä¸ªæ•°æ®ç‚¹å¯ä»¥ç©ä¸¤è½®ï¼Œè§’è‰²äº’æ¢
for i, game_data in enumerate(all_game_rounds_data):
    for turn in range(2):
        round_idx = i * 2 + turn + 1

        # è§’è‰²åˆ†é…
        if turn == 0:
            # è½®æ¬¡ 1: GPT-2 æ˜¯ Speaker, CPM æ˜¯ Listener
            speaker_agent, listener_agent = agent_gpt2, agent_cpm
            description = game_data['description_english']
            correct_option = game_data['concept_chinese']
            distractors = game_data['distractors_chinese']
        else:
            # è½®æ¬¡ 2: CPM æ˜¯ Speaker, GPT-2 æ˜¯ Listener
            speaker_agent, listener_agent = agent_cpm, agent_gpt2
            description = game_data['description_chinese']
            correct_option = game_data['concept_english']
            distractors = game_data['distractors_english']

        print(f"\n--- æ¸¸æˆå›åˆ {round_idx} ---")
        print(f"ğŸ—£ï¸ Speaker ({speaker_agent.name}) æè¿°: \"{description}\"")

        # å‡†å¤‡é€‰é¡¹å¹¶éšæœºæ’åº
        options = distractors + [correct_option]
        random.shuffle(options)
        correct_index = options.index(correct_option)
        print(f"ğŸ‘‚ Listener ({listener_agent.name}) é€‰é¡¹: {options}")

        # --- äº¤äº’ä¸è®¡ç®— ---
        desc_embedding = listener_agent.get_semantic_embedding(description)
        option_embeddings = listener_agent.get_candidate_embeddings(options)

        similarities = F.cosine_similarity(desc_embedding, option_embeddings, dim=1)
        predicted_index = torch.argmax(similarities).item()

        # --- åé¦ˆä¸å­¦ä¹  ---
        is_correct = (predicted_index == correct_index)
        correct_index_tensor = torch.tensor([correct_index], device=device)

        # è®¡ç®— Listener çš„é€‰æ‹©æŸå¤±
        listener_loss = F.cross_entropy(similarities.unsqueeze(0), correct_index_tensor)

        if is_correct:
            final_listener_loss = listener_loss * (1 - REWARD_CORRECT)
            outcome_message = f"ğŸ‰ {listener_agent.name} çŒœå¯¹äº†!"
            correct_predictions_count += 1
        else:
            final_listener_loss = listener_loss * PENALTY_WRONG
            outcome_message = f"ğŸ’” {listener_agent.name} çŒœé”™äº†! (æ­£ç¡®ç­”æ¡ˆ: {correct_option})"
            speaker_agent.error_log.append({"description": description, "listener_chose": options[predicted_index], "correct": correct_option, "round": round_idx})

        # è®¡ç®— Speaker çš„å¯¹é½æŸå¤± (æ— è®ºå¯¹é”™ï¼Œéƒ½ä»¥æ­£ç¡®ç­”æ¡ˆä¸ºå¼•å¯¼)
        speaker_desc_embedding = speaker_agent.get_semantic_embedding(description)
        # ä½¿ç”¨ Listener ç¼–ç æ­£ç¡®ç­”æ¡ˆçš„æ–‡æœ¬ï¼Œä½œä¸ºâ€œç›®æ ‡è¯­ä¹‰â€
        target_semantic_embedding = listener_agent.get_semantic_embedding(correct_option).detach()
        alignment_loss = -F.cosine_similarity(speaker_desc_embedding, target_semantic_embedding).mean()

        # åˆå¹¶æŸå¤±
        total_loss = final_listener_loss + ALIGNMENT_LOSS_WEIGHT * alignment_loss
        total_loss_sum += total_loss.item()

        # åå‘ä¼ æ’­å¹¶æ›´æ–°ä¸¤ä¸ª Agent
        speaker_agent.optimizer.zero_grad()
        listener_agent.optimizer.zero_grad()
        total_loss.backward()
        speaker_agent.optimizer.step()
        listener_agent.optimizer.step()

        # --- æ‰“å°å’Œè®°å½• ---
        print(outcome_message)
        print(f"ğŸ”® ç›¸ä¼¼åº¦: {[f'{s:.3f}' for s in similarities.tolist()]}, é¢„æµ‹: {predicted_index}, æ­£ç¡®: {correct_index}")
        print(f"ğŸ“‰ æ€»æŸå¤±: {total_loss.item():.4f} (Listener Loss: {final_listener_loss.item():.4f}, Speaker Alignment Loss: {alignment_loss.item():.4f})")

        training_log.append({"round_idx": round_idx, "speaker": speaker_agent.name, "listener": listener_agent.name, "is_correct": is_correct, "total_loss": total_loss.item()})


# --- 6. è®­ç»ƒç»“æŸï¼Œæ±‡æ€»ä¸ä¿å­˜ ---
print("\n--- è®­ç»ƒæ€»ç»“ ---")
total_interactions = len(all_game_rounds_data) * 2
final_accuracy = (correct_predictions_count / total_interactions) * 100 if total_interactions > 0 else 0
print(f"æ€»äº¤äº’è½®æ•°: {total_interactions}")
print(f"æ€»çŒœå¯¹æ¬¡æ•°: {correct_predictions_count}")
print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.2f}%")
print(f"å¹³å‡æŸå¤±: {total_loss_sum / total_interactions:.4f}")

# ä¿å­˜è¯¦ç»†ç»“æœ
output_data = {
    "config": {"shared_dim": SHARED_EMBEDDING_DIM, "gpt2_lr": LEARNING_RATE_GPT2, "cpm_lr": LEARNING_RATE_CPM, "alignment_weight": ALIGNMENT_LOSS_WEIGHT},
    "summary": {"accuracy": final_accuracy, "avg_loss": total_loss_sum / total_interactions},
    "gpt2_error_log": agent_gpt2.error_log,
    "cpm_error_log": agent_cpm.error_log,
    "detailed_log": training_log
}
output_file_path = os.path.join(OUTPUT_DIR, "final_training_results.json")
with open(output_file_path, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"\nğŸ‰ å®Œæ•´è®­ç»ƒæ—¥å¿—å’Œç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")
