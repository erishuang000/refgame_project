# âœ… æœ€ç»ˆä¿®æ­£ç‰ˆä»£ç ï¼Œé™„å¸¦æ¸…æ™°çš„ã€ç”¨äºè°ƒè¯•å’Œåˆ†æçš„è¯¦ç»†è¿‡ç¨‹æ‰“å°
# 1. ã€å·²ä¿®æ­£ã€‘ç¡®ä¿ GPT-2 Tokenizer èƒ½å¤„ç†ä¸­æ–‡ï¼Œä½¿å…¶èƒ½æ‰®æ¼” Listener è§’è‰²
# 2. ã€å·²ä¿®æ­£ã€‘ç¡®ä¿ Speaker Agent ä½¿ç”¨è‡ªå·±è¯­è¨€çš„æ¦‚å¿µç”Ÿæˆæè¿°
# 3. ã€æ‰“å°å¢å¼ºã€‘å¯¹æ‰€æœ‰äº¤äº’å’Œè®¡ç®—æ­¥éª¤ï¼Œéƒ½åŠ å…¥äº†æ¸…æ™°çš„æ‰“å°è¯­å¥

import torch
import torch.nn as nn
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoModelForCausalLM
import json
import torch.nn.functional as F
from torch.optim import AdamW
import os
import random

# --- å…¨å±€é…ç½® ---
# â—ï¸ è¯·æ ¹æ®æ‚¨çš„ç¯å¢ƒä¿®æ”¹è·¯å¾„
GPT2_MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/gpt2"
CPM_MODEL_PATH = "/ubsnhome/23063003r/refgame_project/models/CPM-Generate"
DATA_FILE = "/ubsnhome/23063003r/refgame_project/data/bilingual_game_data.json"
OUTPUT_DIR = "/ubsnhome/23063003r/refgame_project/output/"

# --- è¶…å‚æ•°é…ç½® ---
SHARED_EMBEDDING_DIM = 768
LEARNING_RATE_GPT2 = 5e-6
LEARNING_RATE_CPM = 2e-6
REWARD_CORRECT = 0.1
PENALTY_WRONG = 1.1
ALIGNMENT_LOSS_WEIGHT = 0.5
ANTI_ALIGNMENT_WEIGHT = 0.5
CONTRASTIVE_LOSS_WEIGHT = 0.5
CONTRASTIVE_TEMPERATURE = 0.1

# --- Agent ç±»å®šä¹‰ ---
class Agent:
    def __init__(self, model_name, model, tokenizer, learning_rate, device):
        self.name = model_name
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        if hasattr(model.config, 'hidden_size'): native_hidden_size = model.config.hidden_size
        elif hasattr(model.config, 'n_embd'): native_hidden_size = model.config.n_embd
        else: raise ValueError(f"Cannot determine hidden size for model {self.name}")
        self.projection = nn.Linear(native_hidden_size, SHARED_EMBEDDING_DIM).to(self.device)
        self.optimizer = AdamW(list(self.model.parameters()) + list(self.projection.parameters()), lr=learning_rate)
        self.error_log = []

    def get_semantic_embedding(self, text: str):
        self.model.eval()
        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(self.device)
            outputs = self.model(**inputs)
        self.model.train()
        if hasattr(outputs, 'last_hidden_state'): raw_embedding = outputs.last_hidden_state[:, 0, :]
        elif hasattr(outputs, 'logits'): raw_embedding = self.model.transformer(inputs['input_ids'])[0][:,0,:]
        else: raw_embedding = outputs[0][:, 0, :]
        return self.projection(raw_embedding)

    def get_candidate_embeddings(self, sentences: list):
        embeddings = [self.get_semantic_embedding(s) for s in sentences]
        return torch.cat(embeddings, dim=0)

    def train_mode(self):
        self.model.train()
        self.projection.train()

# --- è¾…åŠ©å‡½æ•° ---
def generate_description(concept: str, agent: Agent, max_tokens=30) -> str:
    # --- ã€â€¼ï¸ å…³é”®ä¿®æ­£ â€¼ï¸ã€‘æ ¹æ®Agentçš„ç±»å‹ï¼ŒåŠ¨æ€é€‰æ‹©æ›´ä¼˜çš„Promptæ ¼å¼ ---
    if agent.name == "GPT-2":
        # GPT-2 å¯¹è¿™ç§æŒ‡ä»¤å¼Promptå“åº”è‰¯å¥½
        prompt = f"A short description of the concept '{concept}' is:"
    elif agent.name == "CPM":
        # å¯¹äºCPMï¼Œä½¿ç”¨æ›´åƒâ€œæ–‡æœ¬è¡¥å…¨â€çš„æ ¼å¼ï¼Œè¯±å¯¼å…¶è¿›è¡Œæè¿°
        # æ ¼å¼1ï¼šæœ€ç®€å•çš„è¡¥å…¨
        prompt = f"{concept}æ˜¯"
        # æ ¼å¼2ï¼šç¨å¾®ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
        # prompt = f"å…³äºâ€œ{concept}â€çš„ç®€å•ä»‹ç»ï¼š{concept}"
    else:
        prompt = f"Describe: {concept}\n"

    input_ids = agent.tokenizer(prompt, return_tensors="pt").input_ids.to(agent.device)
    input_length = input_ids.shape[1] # è®°å½•è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦

    agent.model.eval()
    output_ids = agent.model.generate(
        input_ids,
        max_new_tokens=max_tokens,
        do_sample=True,
        top_k=50,
        temperature=0.7,
        pad_token_id=agent.tokenizer.eos_token_id
    )
    agent.model.train()

    # --- ã€â€¼ï¸ å…³é”®ä¿®æ­£ â€¼ï¸ã€‘ä½¿ç”¨æ›´é²æ£’çš„æ–¹å¼è§£ç ï¼Œé¿å…å¤è¿°Prompt ---
    # åªè§£ç ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å…¨éƒ¨
    generated_ids = output_ids[0][input_length:]
    description = agent.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

    return description if description else f"ä¸€ä¸ªå«åš {concept} çš„ä¸œè¥¿"
    
# --- åˆå§‹åŒ–æµç¨‹ ---
os.makedirs(OUTPUT_DIR, exist_ok=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- æ‰©å±• GPT-2 Tokenizer ä»¥å¤„ç†ä¸­æ–‡ ---
print("Loading data to collect Chinese vocabulary for GPT-2...")
with open(DATA_FILE, 'r', encoding='utf-8') as f:
    data = json.load(f)

all_chinese_words = set()
for sample in data:
    all_chinese_words.add(sample['concept_chinese'])
    all_chinese_words.update(sample['distractors_chinese'])
all_chinese_chars = set(''.join(list(all_chinese_words)))
new_chinese_tokens = list(all_chinese_words.union(all_chinese_chars))
print(f"Found {len(new_chinese_tokens)} unique Chinese words/characters to add to GPT-2 tokenizer.")

# åˆå§‹åŒ– GPT-2 Agent
print("Loading and configuring GPT-2...")
gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_PATH)
if gpt2_tokenizer.pad_token is None:
    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token

gpt2_tokenizer.add_tokens(new_chinese_tokens)
print(f"GPT-2 tokenizer vocabulary extended. New size: {len(gpt2_tokenizer)}")

gpt2_model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL_PATH).to(device)
gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))
print("GPT-2 model embedding layer resized.")

gpt2_agent = Agent("GPT-2", gpt2_model, gpt2_tokenizer, LEARNING_RATE_GPT2, device)
gpt2_agent.train_mode()

# åˆå§‹åŒ– CPM Agent
print("Loading CPM...")
cpm_tokenizer = AutoTokenizer.from_pretrained(CPM_MODEL_PATH)
cpm_model_full = AutoModelForCausalLM.from_pretrained(CPM_MODEL_PATH).to(device)
cpm_agent = Agent("CPM", cpm_model_full, cpm_tokenizer, LEARNING_RATE_CPM, device)
cpm_agent.train_mode()

# --- ä¸»è®­ç»ƒå¾ªç¯ (é™„å¸¦è¯¦ç»†æ‰“å°) ---
log = []
total_loss_sum = 0.0
correct_count = 0

print(f"\n--- Starting {len(data) * 2} rounds of interaction (Verbose Mode) ---")
for i, sample in enumerate(data):
    for turn in range(2):
        round_idx = i * 2 + turn + 1
        print(f"\n\n{'='*25} æ¸¸æˆå›åˆ {round_idx} {'='*25}")

        # --- é˜¶æ®µ 1: è§’è‰²åˆ†é…ä¸ä»»åŠ¡è®¾ç½® ---
        print("\nâ–¶ï¸  é˜¶æ®µ 1: è§’è‰²åˆ†é…ä¸ä»»åŠ¡è®¾ç½®")
        if turn == 0:
            speaker, listener = gpt2_agent, cpm_agent
            concept_for_speaker = sample['concept_english']
            correct_option_for_listener = sample['concept_chinese']
            options_for_listener = sample['distractors_chinese'] + [correct_option_for_listener]
        else:
            speaker, listener = cpm_agent, gpt2_agent
            concept_for_speaker = sample['concept_chinese']
            correct_option_for_listener = sample['concept_english']
            options_for_listener = sample['distractors_english'] + [correct_option_for_listener]
        print(f"    ğŸ—£ï¸  Speaker: {speaker.name}")
        print(f"    ğŸ‘‚  Listener: {listener.name}")

        # --- é˜¶æ®µ 2: Speaker ç”Ÿæˆæè¿° ---
        print("\nâ–¶ï¸  é˜¶æ®µ 2: Speaker ç”Ÿæˆæè¿°")
        print(f"    Speakeræ¥æ”¶åˆ°çš„æ¦‚å¿µ: '{concept_for_speaker}'")
        random.shuffle(options_for_listener)
        correct_index = options_for_listener.index(correct_option_for_listener)
        description = generate_description(concept_for_speaker, speaker)
        print(f"    Speakerç”Ÿæˆçš„æè¿°: '{description}'")

        # --- é˜¶æ®µ 3: Listener ç†è§£ä¸çŒœæµ‹ ---
        print("\nâ–¶ï¸  é˜¶æ®µ 3: Listener ç†è§£ä¸çŒœæµ‹")
        print(f"    Listeneræ¥æ”¶åˆ°çš„é€‰é¡¹: {options_for_listener}")
        print(f"    (æœ¬è½®æ­£ç¡®ç­”æ¡ˆæ˜¯ç´¢å¼• {correct_index}: '{correct_option_for_listener}')")
        desc_embed = listener.get_semantic_embedding(description)
        opt_embeds = listener.get_candidate_embeddings(options_for_listener)
        sims = F.cosine_similarity(desc_embed, opt_embeds, dim=1)
        pred_idx = torch.argmax(sims).item()

        print("    è®¡ç®—å‡ºçš„ç›¸ä¼¼åº¦:")
        for k, sim_val in enumerate(sims.tolist()):
            print(f"        - é€‰é¡¹ '{options_for_listener[k]}': {sim_val:.4f}")

        print(f"    Listenerçš„é€‰æ‹©: ç´¢å¼• {pred_idx} -> '{options_for_listener[pred_idx]}'")
        is_correct = (pred_idx == correct_index)
        if is_correct:
            print("    âœ… ç»“è®º: çŒœæµ‹æ­£ç¡®ï¼")
        else:
            print("    âŒ ç»“è®º: çŒœæµ‹é”™è¯¯ï¼")

        # --- é˜¶æ®µ 4: å¤åˆæŸå¤±è®¡ç®— (è¯¦ç»†åˆ†è§£) ---
        print("\nâ–¶ï¸  é˜¶æ®µ 4: å¤åˆæŸå¤±è®¡ç®— (è¯¦ç»†åˆ†è§£)")

        # A. Listener Loss
        correct_tensor = torch.tensor([correct_index], device=device)
        listener_loss = F.cross_entropy(sims.unsqueeze(0), correct_tensor)
        reward_penalty_factor = (1 - REWARD_CORRECT) if is_correct else PENALTY_WRONG
        final_listener_loss = listener_loss * reward_penalty_factor
        print(f"    [A] Listener Loss:")
        print(f"        - åŸºç¡€äº¤å‰ç†µæŸå¤±: {listener_loss.item():.4f}")
        print(f"        - å¥–æƒ©ç³»æ•°: {reward_penalty_factor:.2f} ({'å¥–åŠ±' if is_correct else 'æƒ©ç½š'})")
        print(f"        -  => æœ€ç»ˆ Listener Loss = {final_listener_loss.item():.4f}")

        # è·å–ç”¨äº Speaker å­¦ä¹ çš„åµŒå…¥å‘é‡
        speaker_embed = speaker.get_semantic_embedding(description)
        target_embed = listener.get_semantic_embedding(correct_option_for_listener).detach()

        # B. Alignment Loss (å¸å¼•åŠ›)
        alignment_loss = -F.cosine_similarity(speaker_embed, target_embed).mean()
        print(f"    [B] Alignment Loss (å¸å¼•åŠ›):")
        print(f"        - ç›®æ ‡: æ‹‰è¿‘'{description[:20]}...'å’Œ'{correct_option_for_listener}'çš„è¯­ä¹‰è·ç¦»")
        print(f"        -  => è®¡ç®—å‡ºçš„ Alignment Loss = {alignment_loss.item():.4f}")

        # C. Anti-Alignment Loss (æ’æ–¥åŠ›)
        anti_loss = torch.tensor(0.0, device=device)
        print(f"    [C] Anti-Alignment Loss (æ’æ–¥åŠ›):")
        if not is_correct:
            misleading_option = options_for_listener[pred_idx]
            misleading_embed = listener.get_semantic_embedding(misleading_option).detach()
            anti_loss = F.cosine_similarity(speaker_embed, misleading_embed).mean()
            print(f"        - ç›®æ ‡: æ¨è¿œ'{description[:20]}...'å’Œè¢«é”™é€‰çš„'{misleading_option}'çš„è¯­ä¹‰è·ç¦»")
            print(f"        -  => è®¡ç®—å‡ºçš„ Anti-Alignment Loss = {anti_loss.item():.4f}")
        else:
            print("        - (çŒœæµ‹æ­£ç¡®ï¼Œæ— éœ€è®¡ç®—æ­¤é¡¹æŸå¤±)")

        # D. Contrastive Loss (åŒºåˆ†åŠ›)
        print(f"    [D] Contrastive Loss (åŒºåˆ†åŠ›):")
        negative_options = [opt for opt in options_for_listener if opt != correct_option_for_listener]
        if negative_options:
            negative_embeds = listener.get_candidate_embeddings(negative_options).detach()
            pos_sim = F.cosine_similarity(speaker_embed, target_embed) / CONTRASTIVE_TEMPERATURE
            neg_sims = F.cosine_similarity(speaker_embed, negative_embeds) / CONTRASTIVE_TEMPERATURE
            all_logits = torch.cat([pos_sim, neg_sims.view(-1)])
            contrastive_labels = torch.tensor([0], device=device)
            contrastive_loss = F.cross_entropy(all_logits.unsqueeze(0), contrastive_labels)
            print(f"        - ç›®æ ‡: è®©'{description[:20]}...'ä¸'{correct_option_for_listener}'çš„ç›¸ä¼¼åº¦è¿œé«˜äºå…¶ä»–æ‰€æœ‰é€‰é¡¹")
            print(f"        -  => è®¡ç®—å‡ºçš„ Contrastive Loss = {contrastive_loss.item():.4f}")
        else:
            contrastive_loss = torch.tensor(0.0, device=device)
            print("        - (æ²¡æœ‰è´Ÿæ ·æœ¬ï¼Œæ— éœ€è®¡ç®—æ­¤é¡¹æŸå¤±)")

        # E. åˆå¹¶æ€»æŸå¤±
        print(f"    [E] æ€»æŸå¤±è®¡ç®—:")
        total_loss = final_listener_loss + \
                     ALIGNMENT_LOSS_WEIGHT * alignment_loss + \
                     ANTI_ALIGNMENT_WEIGHT * anti_loss + \
                     CONTRASTIVE_LOSS_WEIGHT * contrastive_loss
        print("        " + "-"*40)
        print(f"        Total Loss = Listener_Loss + (W_align * Align_L) + (W_anti * Anti_L) + (W_contrast * Contrast_L)")
        print(f"                   = {final_listener_loss.item():.4f} + ({ALIGNMENT_LOSS_WEIGHT} * {alignment_loss.item():.4f}) + ({ANTI_ALIGNMENT_WEIGHT} * {anti_loss.item():.4f}) + ({CONTRASTIVE_LOSS_WEIGHT} * {contrastive_loss.item():.4f})")
        print(f"                   = {final_listener_loss.item():.4f} + {ALIGNMENT_LOSS_WEIGHT*alignment_loss.item():.4f} + {ANTI_ALIGNMENT_WEIGHT*anti_loss.item():.4f} + {CONTRASTIVE_LOSS_WEIGHT*contrastive_loss.item():.4f}")
        print(f"        ----------------------------------------")
        print(f"        ==> ğŸ’¸ æœ€ç»ˆæ€»æŸå¤± (Final Total Loss): {total_loss.item():.4f}")

        # 4. ååŒæ›´æ–°
        print("\nâ–¶ï¸  é˜¶æ®µ 5: æ¨¡å‹ååŒæ›´æ–°")
        print("    æ‰§è¡Œ total_loss.backward() è®¡ç®—ä¸¤ä¸ªAgentæ‰€æœ‰ç›¸å…³å‚æ•°çš„æ¢¯åº¦...")
        speaker.optimizer.zero_grad()
        listener.optimizer.zero_grad()
        total_loss.backward()
        print(f"    æ‰§è¡Œ speaker.optimizer.step() æ›´æ–° {speaker.name} çš„æƒé‡...")
        speaker.optimizer.step()
        print(f"    æ‰§è¡Œ listener.optimizer.step() æ›´æ–° {listener.name} çš„æƒé‡...")
        listener.optimizer.step()
        print("    âœ… æ›´æ–°å®Œæˆ!")

        # 5. æ—¥å¿—è®°å½•
        total_loss_sum += total_loss.item()
        if is_correct:
            correct_count += 1
        else:
            speaker.error_log.append({
                "round": round_idx,
                "speaker_concept": concept_for_speaker,
                "description": description,
                "listener_chose": options_for_listener[pred_idx],
                "correct_answer": correct_option_for_listener
            })
        log.append({"round": round_idx, "speaker": speaker.name, "correct": is_correct, "loss": total_loss.item()})

# --- æ€»ç»“ä¸ä¿å­˜ ---
total_games = len(data) * 2
accuracy = (correct_count / total_games * 100) if total_games > 0 else 0
avg_loss = total_loss_sum / total_games if total_games > 0 else 0

summary = {
    "config": {
        "shared_dim": SHARED_EMBEDDING_DIM, "gpt2_lr": LEARNING_RATE_GPT2, "cpm_lr": LEARNING_RATE_CPM,
        "alignment_weight": ALIGNMENT_LOSS_WEIGHT, "anti_align_weight": ANTI_ALIGNMENT_WEIGHT,
        "contrastive_weight": CONTRASTIVE_LOSS_WEIGHT, "temperature": CONTRASTIVE_TEMPERATURE
    },
    "summary": {"accuracy": accuracy, "avg_loss": avg_loss},
    "gpt2_error_log": gpt2_agent.error_log,
    "cpm_error_log": cpm_agent.error_log,
    "log": log
}

output_file_path = os.path.join(OUTPUT_DIR, "final_training_run_results_verbose.json")
with open(output_file_path, 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

print(f"\n\n{'='*25} è®­ç»ƒç»“æŸ {'='*25}")
print(f"âœ… å‡†ç¡®ç‡: {accuracy:.2f}% | å¹³å‡æŸå¤±: {avg_loss:.4f}")
print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file_path}")
