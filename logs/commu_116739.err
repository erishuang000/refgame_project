The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Traceback (most recent call last):
  File "/hpc2/puhome/23063003r/refgame_project/scripts/commu_gpt_newLoss.py", line 314, in <module>
    trainer.train()
  File "/hpc2/puhome/23063003r/refgame_project/scripts/commu_gpt_newLoss.py", line 251, in train
    self.train_one_round(single_game_round, i + 1, total_rounds)
  File "/hpc2/puhome/23063003r/refgame_project/scripts/commu_gpt_newLoss.py", line 162, in train_one_round
    self.model(
  File "/puhome/23063003r/.conda/envs/refgame/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/puhome/23063003r/.conda/envs/refgame/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hpc2/puhome/23063003r/refgame_project/scripts/commu_gpt_newLoss.py", line 94, in forward
    processed_cn_inputs = self.tokenizer(
  File "/puhome/23063003r/.conda/envs/refgame/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2867, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/puhome/23063003r/.conda/envs/refgame/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2955, in _call_one
    return self.batch_encode_plus(
  File "/puhome/23063003r/.conda/envs/refgame/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3147, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/puhome/23063003r/.conda/envs/refgame/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2769, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
